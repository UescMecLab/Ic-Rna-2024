{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "def benchmark(x1, x2):\n",
    "    return (    (1.3356 * (1.5 * (1 - x1))) \n",
    "                + (np.exp((2 * x1) - 1) * np.sin((3 * np.pi) * ((x1 - 0.6) ** 2)))\n",
    "                + (np.exp(3 * (x2 - 0.5)) * np.sin((4 * np.pi) * ((x2 - 0.9) ** 2)))\n",
    "            )\n",
    "\n",
    "def getData(grid):\n",
    "    x1 = np.linspace(0, 1, grid)\n",
    "    x2 = np.linspace(0, 1, grid)\n",
    "    x1, x2 = np.meshgrid(x1, x2)\n",
    "    f_x1_x2 = benchmark(x1, x2)\n",
    "    data = {'x1': x1.flatten(), 'x2': x2.flatten(), 'f(x1,x2)': f_x1_x2.flatten()}\n",
    "    df = pd.DataFrame(data)\n",
    "    return df\n",
    "\n",
    "\n",
    "df_1000 = getData(32)\n",
    "df_test = pd.read_excel(\"../Kriging-data.xlsx\", sheet_name=\"Test\")\n",
    "df_training = pd.read_excel(\"../Kriging-data.xlsx\", sheet_name=\"Training\")\n",
    "\n",
    "exponential = pd.read_excel(\"../VirtualSamples.xlsx\", sheet_name=\"Exponential\")\n",
    "spherical = pd.read_excel(\"../VirtualSamples.xlsx\", sheet_name=\"Spherical\")\n",
    "gaussian = pd.read_excel(\"../VirtualSamples.xlsx\", sheet_name=\"Gaussian\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "scaler = StandardScaler()\n",
    "out_scaler = StandardScaler()\n",
    "\n",
    "def show_norm(df, label=\"data\", plot=False):\n",
    "    df_norm = pd.DataFrame(scaler.transform(df), columns=df.columns)\n",
    "    df_denorm = pd.DataFrame(scaler.inverse_transform(df_norm), columns=df_norm.columns)\n",
    "\n",
    "    if (plot):\n",
    "        df.plot(title=f\"{label}: Original data\")\n",
    "        df_norm.plot(title=f\"{label}: Normalized data\")\n",
    "        df_denorm.plot(title=f\"{label}: Denormalized data\")\n",
    "    return (df_norm)\n",
    "\n",
    "\n",
    "def test_out_scaler(df):\n",
    "    out = df[\"f(x1,x2)\"].values.reshape(-1, 1)  \n",
    "    plt.plot(out, label='Original')\n",
    "    out_scaler.fit(out)\n",
    "    norm = out_scaler.transform(out)\n",
    "    plt.plot(norm, label='Normalizado')\n",
    "    plt.plot(out_scaler.inverse_transform(norm), label='desnormalizado')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "scaler.fit(df_training)\n",
    "test_out_scaler(df_training)\n",
    "\n",
    "df_training_norm = show_norm(df_training, \"Training\")\n",
    "df_1000_norm = show_norm(df_1000)\n",
    "df_exponential_norm = show_norm(pd.concat([df_training, exponential]))\n",
    "df_spherical_norm = show_norm(pd.concat([df_training, spherical]))\n",
    "df_gaussian_norm = show_norm(pd.concat([df_training, gaussian]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.chdir(\"./content\")\n",
    "\n",
    "lm_dir = \"tf-levenberg-marquardt\"\n",
    "if not os.path.exists(lm_dir):\n",
    "  !git clone https://github.com/fabiodimarco/$lm_dir\n",
    "\n",
    "os.chdir(lm_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_df(df):\n",
    "    _input = np.vstack([df['x1'], df['x2']]).T\n",
    "    _output = np.array(df['f(x1,x2)'])\n",
    "    return (_input, _output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from keras import regularizers\n",
    "from keras import initializers\n",
    "import levenberg_marquardt as lm\n",
    "\n",
    "# layers, neurons\n",
    "class ShuffleArchitecture:\n",
    "    def __init__(self, input_size, hidden_sizes, output_size, act_h, act_o, param_reg):\n",
    "        self.input_size = input_size\n",
    "        self.hidden_sizes = hidden_sizes\n",
    "        self.output_size = output_size\n",
    "        self.act_h = act_h\n",
    "        self.act_o = act_o\n",
    "        self.regularizer = regularizers.L2(param_reg)\n",
    "        self.initializer = initializers.RandomUniform(minval=-0.5, maxval=0.5, seed=np.random.randint(1, 10000))\n",
    "\n",
    "    def compute_k(self):\n",
    "        total_parameters = 0\n",
    "        for layer in self.model.layers:\n",
    "            weights = layer.get_weights()\n",
    "            if len(weights) > 0:  \n",
    "                for w in weights:\n",
    "                    total_parameters += np.prod(w.shape)\n",
    "        return total_parameters\n",
    "        \n",
    "    def set_architecture(self):\n",
    "        self.model = tf.keras.Sequential()\n",
    "        self.model.add(tf.keras.layers.Dense(self.hidden_sizes[0],\n",
    "                        input_shape=(self.input_size,),\n",
    "                        activation=self.act_h,\n",
    "                        kernel_regularizer=self.regularizer,\n",
    "                        kernel_initializer=self.initializer,                        \n",
    "                        ))  # input layer\n",
    "\n",
    "        for size in self.hidden_sizes[1:]:  # hidden layers\n",
    "            self.model.add(tf.keras.layers.Dense(size,\n",
    "                            activation=self.act_h,\n",
    "                            kernel_regularizer=self.regularizer,\n",
    "                            kernel_initializer=self.initializer,  \n",
    "                        ))\n",
    "\n",
    "        self.model.add(tf.keras.layers.Dense(self.output_size,\n",
    "                        activation=self.act_o,\n",
    "                        kernel_regularizer=self.regularizer,\n",
    "                        kernel_initializer=self.initializer,  \n",
    "                        ))  # output layer\n",
    "\n",
    "    def create_model(self, _learning_rate):\n",
    "        self.model.compile(\n",
    "            optimizer=tf.keras.optimizers.Adam(learning_rate=_learning_rate),\n",
    "            loss=tf.keras.losses.MeanSquaredError())\n",
    "\n",
    "        self.lm_model = lm.ModelWrapper(\n",
    "            tf.keras.models.clone_model(self.model))\n",
    "\n",
    "        self.lm_model.compile(\n",
    "            optimizer=tf.keras.optimizers.SGD(learning_rate=_learning_rate),\n",
    "            loss=lm.MeanSquaredError())\n",
    "        return(self.lm_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from sklearn.metrics import r2_score, mean_squared_error, root_mean_squared_error, mean_absolute_percentage_error \n",
    "\n",
    "class Rebuild:\n",
    "    def __init__(self):\n",
    "        self.df = pd.read_excel(f\"../../better_results.xlsx\")   \n",
    "\n",
    "    def LoadModelWeights(self, model, fileName):        \n",
    "        path = f\"../models/{fileName}.keras\"\n",
    "        model.load_weights(path)\n",
    "\n",
    "    def LoadPrevDataset(self, fileName):\n",
    "        fileName = fileName.replace(\"model\", \"dataset\")\n",
    "        pos = fileName.rfind(\"_\")\n",
    "        path = f\"../dataset/{fileName[:pos]}.pkl\" \n",
    "        with open(path, 'rb') as f:\n",
    "            _data, _train, _vt, _val, _test = pickle.load(f)\n",
    "        self._data, self._train, self._vt, self._val, self._test = _data, _train, _vt, _val, _test    \n",
    "    \n",
    "    def setArchitecture(self, _hidden_sizes, _pg, _lr):\n",
    "        shuffler = ShuffleArchitecture(input_size  = 2,\n",
    "                                        hidden_sizes = _hidden_sizes,\n",
    "                                        output_size = 1,\n",
    "                                        act_h = 'tanh',\n",
    "                                        act_o = 'linear',\n",
    "                                        param_reg=_pg)\n",
    "        shuffler.set_architecture()\n",
    "        self.k = shuffler.compute_k()\n",
    "        return(shuffler.create_model(_lr))            \n",
    "\n",
    "    def getArchitecture(self, architecture):\n",
    "        hidden_size = [int(x) for x in architecture.split(\"[\")[1].split(\"]\")[0].split(\", \")]\n",
    "        regularizer = float(architecture.split(\"regularizer=\")[1].split(\",\")[0])\n",
    "        learning_rate = float(architecture.split(\"learning_rate=\")[1])\n",
    "        return hidden_size, regularizer, learning_rate\n",
    "\n",
    "    def PlotResults(self, original_values, predicted_values):\n",
    "        titles = ['Original vs. Preditos (Completo)',\n",
    "                    'Original vs. Preditos (Teste)',\n",
    "                    'Original vs. Preditos (Validação)',\n",
    "                    'Original vs. Preditos (Validação e Teste)']\n",
    "        fig, axes = plt.subplots(2, 2, figsize=(12, 8))\n",
    "        for i, ax in enumerate(axes.flat):\n",
    "            ax.plot(original_values[i], label='Original')\n",
    "            ax.plot(predicted_values[i], label='Preditos')\n",
    "            ax.set_title(titles[i])\n",
    "            ax.legend()\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "    def predictResults(self):\n",
    "        pred = self.lm_model.predict(self._data[0]).flatten()\n",
    "        test_pred = self.lm_model.predict(self._test[0]).flatten()\n",
    "        val_pred = self.lm_model.predict(self._val[0]).flatten()\n",
    "        vt_pred = self.lm_model.predict(self._vt[0]).flatten()\n",
    "\n",
    "        # Calculando as metricas com a saida desnormalizada\n",
    "        pred_denorm = out_scaler.inverse_transform(pred.reshape(-1, 1))\n",
    "        test_pred_denorm = out_scaler.inverse_transform(test_pred.reshape(-1, 1))\n",
    "        val_pred_denorm = out_scaler.inverse_transform(val_pred.reshape(-1, 1))\n",
    "        vt_pred_denorm = out_scaler.inverse_transform(vt_pred.reshape(-1, 1))\n",
    "\n",
    "        out_denorm = out_scaler.inverse_transform(self._data[1].reshape(-1, 1))\n",
    "        test_denorm = out_scaler.inverse_transform(self._test[1].reshape(-1, 1))\n",
    "        val_denorm = out_scaler.inverse_transform(self._val[1].reshape(-1, 1))\n",
    "        vt_denorm = out_scaler.inverse_transform(self._vt[1].reshape(-1, 1))\n",
    "        \n",
    "        original_values = [out_denorm, test_denorm, val_denorm, vt_denorm]\n",
    "        predicted_values = [pred_denorm, test_pred_denorm, val_pred_denorm, vt_pred_denorm]\n",
    "        return (original_values, predicted_values)\n",
    "\n",
    "\n",
    "    def rebuildFullNet(self, model):\n",
    "        for file_model, architecture in zip(self.df[\"model\"], self.df[\"Architecture\"]):\n",
    "            if model == file_model:\n",
    "                hidden_size, regularizer, learning_rate = self.getArchitecture(architecture)\n",
    "                self.lm_model = self.setArchitecture(hidden_size, regularizer, learning_rate)\n",
    "                self.LoadModelWeights(self.lm_model, file_model)\n",
    "                self.LoadPrevDataset(file_model)\n",
    "                original_values, predicted_values = self.predictResults()\n",
    "                self.PlotResults(original_values, predicted_values)\n",
    "                metrics = {\n",
    "                          'r2': r2_score(original_values, predicted_values),\n",
    "                          'mse': mean_squared_error(original_values, predicted_values),\n",
    "                          'mape': mean_absolute_percentage_error(original_values, predicted_values),\n",
    "                          'rmse': root_mean_squared_error(original_values, predicted_values),\n",
    "                          }\n",
    "                print(metrics)\n",
    "    \n",
    "\n",
    "    def LoadNewDataset(self):\n",
    "        inputScaler = StandardScaler()\n",
    "        df_orig = df_test\n",
    "        df = pd.DataFrame(inputScaler.fit_transform(df_orig), columns=df_orig.columns)\n",
    "        x1, x2 = df['x1'], df['x2']\n",
    "        input = np.vstack([x1, x2]).T\n",
    "        return (input, df_orig)\n",
    "\n",
    "    def PlotNewDataResults(self, df, label):\n",
    "        columns = df.columns[2:]\n",
    "        for column in columns:\n",
    "            plt.scatter(df.index, df[column], label=column)\n",
    "        plt.xlabel('Amostra')\n",
    "        plt.ylabel('Valores Preditos pela rede')\n",
    "        plt.title(f'Gráfico de Dispersão para {label}')\n",
    "        plt.legend(loc='upper center', bbox_to_anchor=(0.5, -0.05),\n",
    "          fancybox=True, shadow=True, ncol=5)\n",
    "        plt.show()\n",
    "    \n",
    "    def mean_std(self, df):\n",
    "        df_out = df.iloc[:, 2:]\n",
    "        mean = df_out.mean(axis=1)\n",
    "        std = df_out.std(axis=1)\n",
    "        df[\"media\"] = mean\n",
    "        df[\"desvio padrao\"] = std\n",
    "        display(df)\n",
    "        df.to_excel('../../results.xlsx', index=False)\n",
    "        \n",
    "    def TestNewData(self, label=\"\"):\n",
    "        input, df = self.LoadNewDataset()\n",
    "        for file_model, architecture in zip(self.df[\"model\"], self.df[\"Architecture\"]):\n",
    "            hidden_size, regularizer, learning_rate = self.getArchitecture(architecture)\n",
    "            self.lm_model = self.setArchitecture(hidden_size, regularizer, learning_rate)\n",
    "            self.LoadModelWeights(self.lm_model, file_model)\n",
    "            pred = self.lm_model.predict(input).flatten()\n",
    "            pred_denorm = out_scaler.inverse_transform(pred.reshape(-1, 1))\n",
    "            df[file_model] = pred_denorm\n",
    "        self.PlotNewDataResults(df, label)\n",
    "        self.mean_std(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Rebuilder = Rebuild()\n",
    "Rebuilder.TestNewData(label=\"Trained with original Data\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "def benchmark(x1, x2):\n",
    "    return (    (1.3356 * (1.5 * (1 - x1))) \n",
    "                + (np.exp((2 * x1) - 1) * np.sin((3 * np.pi) * ((x1 - 0.6) ** 2)))\n",
    "                + (np.exp(3 * (x2 - 0.5)) * np.sin((4 * np.pi) * ((x2 - 0.9) ** 2)))\n",
    "            )\n",
    "\n",
    "def getData(grid):\n",
    "    x1 = np.linspace(0, 1, grid)\n",
    "    x2 = np.linspace(0, 1, grid)\n",
    "    x1, x2 = np.meshgrid(x1, x2)\n",
    "    f_x1_x2 = benchmark(x1, x2)\n",
    "    data = {'x1': x1.flatten(), 'x2': x2.flatten(), 'f(x1,x2)': f_x1_x2.flatten()}\n",
    "    df = pd.DataFrame(data)\n",
    "    return df\n",
    "\n",
    "\n",
    "df_1000 = getData(32)\n",
    "df_test = pd.read_excel(\"../Kriging-data.xlsx\", sheet_name=\"Test\")\n",
    "df_training = pd.read_excel(\"../Kriging-data.xlsx\", sheet_name=\"Training\")\n",
    "\n",
    "exponential = pd.read_excel(\"../VirtualSamples.xlsx\", sheet_name=\"Exponential\")\n",
    "spherical = pd.read_excel(\"../VirtualSamples.xlsx\", sheet_name=\"Spherical\")\n",
    "gaussian = pd.read_excel(\"../VirtualSamples.xlsx\", sheet_name=\"Gaussian\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "scaler = StandardScaler()\n",
    "out_scaler = StandardScaler()\n",
    "\n",
    "def show_norm(df, label=\"data\", plot=False):\n",
    "    df_norm = pd.DataFrame(scaler.transform(df), columns=df.columns)\n",
    "    df_denorm = pd.DataFrame(scaler.inverse_transform(df_norm), columns=df_norm.columns)\n",
    "\n",
    "    if (plot):\n",
    "        df.plot(title=f\"{label}: Original data\")\n",
    "        df_norm.plot(title=f\"{label}: Normalized data\")\n",
    "        df_denorm.plot(title=f\"{label}: Denormalized data\")\n",
    "    return (df_norm)\n",
    "\n",
    "\n",
    "def test_out_scaler(df):\n",
    "    out = df[\"f(x1,x2)\"].values.reshape(-1, 1)  \n",
    "    plt.plot(out, label='Original')\n",
    "    out_scaler.fit(out)\n",
    "    norm = out_scaler.transform(out)\n",
    "    plt.plot(norm, label='Normalizado')\n",
    "    plt.plot(out_scaler.inverse_transform(norm), label='desnormalizado')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "scaler.fit(df_training)\n",
    "test_out_scaler(df_training)\n",
    "\n",
    "df_training_norm = show_norm(df_training, \"Training\", True)\n",
    "df_1000_norm = show_norm(df_1000)\n",
    "df_exponential_norm = show_norm(pd.concat([df_training, exponential]))\n",
    "df_spherical_norm = show_norm(pd.concat([df_training, spherical]))\n",
    "df_gaussian_norm = show_norm(pd.concat([df_training, gaussian]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.chdir(\"./content\")\n",
    "\n",
    "lm_dir = \"tf-levenberg-marquardt\"\n",
    "if not os.path.exists(lm_dir):\n",
    "  !git clone https://github.com/fabiodimarco/$lm_dir\n",
    "\n",
    "os.chdir(lm_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_df(df):\n",
    "    _input = np.vstack([df['x1'], df['x2']]).T\n",
    "    _output = np.array(df['f(x1,x2)'])\n",
    "    return (_input, _output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-21 18:34:53.235257: I external/local_tsl/tsl/cuda/cudart_stub.cc:31] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2024-07-21 18:34:53.265287: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-07-21 18:34:53.265318: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-07-21 18:34:53.266320: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-07-21 18:34:53.271703: I external/local_tsl/tsl/cuda/cudart_stub.cc:31] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2024-07-21 18:34:53.272573: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-07-21 18:34:54.167650: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from keras import regularizers\n",
    "from keras import initializers\n",
    "import levenberg_marquardt as lm\n",
    "\n",
    "# layers, neurons\n",
    "class ShuffleArchitecture:\n",
    "    def __init__(self, input_size, hidden_sizes, output_size, act_h, act_o, param_reg):\n",
    "        self.input_size = input_size\n",
    "        self.hidden_sizes = hidden_sizes\n",
    "        self.output_size = output_size\n",
    "        self.act_h = act_h\n",
    "        self.act_o = act_o\n",
    "        self.regularizer = regularizers.L2(param_reg)\n",
    "        self.initializer = initializers.RandomUniform(minval=-0.5, maxval=0.5, seed=np.random.randint(1, 10000))\n",
    "\n",
    "    def compute_k(self):\n",
    "        total_parameters = 0\n",
    "        for layer in self.model.layers:\n",
    "            weights = layer.get_weights()\n",
    "            if len(weights) > 0:  \n",
    "                for w in weights:\n",
    "                    total_parameters += np.prod(w.shape)\n",
    "        return total_parameters\n",
    "        \n",
    "    def set_architecture(self):\n",
    "        self.model = tf.keras.Sequential()\n",
    "        self.model.add(tf.keras.layers.Dense(self.hidden_sizes[0],\n",
    "                        input_shape=(self.input_size,),\n",
    "                        activation=self.act_h,\n",
    "                        kernel_regularizer=self.regularizer,\n",
    "                        kernel_initializer=self.initializer,                        \n",
    "                        ))  # input layer\n",
    "\n",
    "        for size in self.hidden_sizes[1:]:  # hidden layers\n",
    "            self.model.add(tf.keras.layers.Dense(size,\n",
    "                            activation=self.act_h,\n",
    "                            kernel_regularizer=self.regularizer,\n",
    "                            kernel_initializer=self.initializer,  \n",
    "                        ))\n",
    "\n",
    "        self.model.add(tf.keras.layers.Dense(self.output_size,\n",
    "                        activation=self.act_o,\n",
    "                        kernel_regularizer=self.regularizer,\n",
    "                        kernel_initializer=self.initializer,  \n",
    "                        ))  # output layer\n",
    "\n",
    "    def create_model(self, _learning_rate):\n",
    "        self.model.compile(\n",
    "            optimizer=tf.keras.optimizers.Adam(learning_rate=_learning_rate),\n",
    "            loss=tf.keras.losses.MeanSquaredError())\n",
    "\n",
    "        self.lm_model = lm.ModelWrapper(\n",
    "            tf.keras.models.clone_model(self.model))\n",
    "\n",
    "        self.lm_model.compile(\n",
    "            optimizer=tf.keras.optimizers.SGD(learning_rate=_learning_rate),\n",
    "            loss=lm.MeanSquaredError())\n",
    "        return(self.lm_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from keras.callbacks import EarlyStopping\n",
    "from sklearn.metrics import r2_score, mean_squared_error, root_mean_squared_error, mean_absolute_percentage_error \n",
    "\n",
    "class TrainWithSmallDataset:\n",
    "    def __init__(self, batch_size=1000):\n",
    "        self.batch_size = batch_size\n",
    "        self.betters = []\n",
    "        self.k = 0\n",
    "\n",
    "    def create_dataset(self, input, output):\n",
    "      input = tf.expand_dims(tf.cast(input, tf.float32), axis=-1)\n",
    "      output = tf.expand_dims(tf.cast(output, tf.float32), axis=-1)\n",
    "      dataset = tf.data.Dataset.from_tensor_slices((input, output))\n",
    "      dataset = dataset.shuffle(len(input))\n",
    "      dataset = dataset.batch(self.batch_size).cache()\n",
    "      dataset = dataset.prefetch(tf.data.experimental.AUTOTUNE)\n",
    "      return (dataset, input, output)\n",
    "\n",
    "    def split_dataset(self, input, output, sup_input, sup_output):\n",
    "      input_train, input_vt, output_train, output_vt = train_test_split(input, output, test_size=0.3, shuffle = True)\n",
    "      input_val, input_test, output_val, output_test = train_test_split(input_vt, output_vt, test_size=0.5, shuffle = True)\n",
    "\n",
    "      self.train_dataset, self.train_input, self.train_output = self.create_dataset(input_train, output_train)\n",
    "      self.val_dataset, self.val_input, self.val_output = self.create_dataset(input_val, output_val)\n",
    "      self.test_dataset, self.test_input, self.test_output = self.create_dataset(input_test, output_test)\n",
    "      self.vt_dataset, self.vt_input, self.vt_output = self.create_dataset(input_vt, output_vt)\n",
    "      self.sup_dataset, self.sup_input, self.sup_output = self.create_dataset(sup_input, sup_output)\n",
    "      self.dataset, self.input, self.output = self.create_dataset(input, output)\n",
    "\n",
    "      self._data = (input, output)\n",
    "      self._train = (input_train, output_train)\n",
    "      self._vt = (input_vt, output_vt)\n",
    "      self._val = (input_val, output_val)\n",
    "      self._test = (input_test, output_test)\n",
    "      self._sup = (sup_input, sup_output)\n",
    "\n",
    "    def train_using_lm(self, train_dataset, epochs=1000):\n",
    "      early_stopping_monitor = EarlyStopping(monitor='val_loss',\n",
    "                                              patience=6,\n",
    "                                              restore_best_weights=True)\n",
    "\n",
    "      self.results = self.lm_model.fit(train_dataset,\n",
    "                                            epochs=epochs,\n",
    "                                            validation_data=self.val_dataset,\n",
    "                                            callbacks=[early_stopping_monitor],\n",
    "                                            verbose=0)\n",
    "      print (\"Stopped at epoch: \", early_stopping_monitor.stopped_epoch)\n",
    "    \n",
    "    def get_new_metrics(self, orig, pred, r2, mse):\n",
    "      n = len(orig) # N: quantidade de saidas\n",
    "      k = self.k\n",
    "      waste = (orig.flatten() - pred.flatten())\n",
    "\n",
    "      mape = mean_absolute_percentage_error(orig, pred)  \n",
    "      r2_adj = 1 - (((n - 1)/(n - k - 1)) * (1 - r2))\n",
    "      rsd = np.sqrt(np.sum(waste ** 2) / (n - 2))\n",
    "      rmse = root_mean_squared_error(orig, pred)          \n",
    "      aic = (-2 * np.log(mse)) + (2 * k)\n",
    "      bic = (-2 * np.log(mse)) + (k * np.log(n))\n",
    "      return (mape, r2_adj, rsd, rmse, aic, bic)\n",
    "      \n",
    "\n",
    "    def get_metrics(self):\n",
    "          # Calculando a saida com os dados normalizados\n",
    "          pred = self.lm_model.predict(self.input).flatten()\n",
    "          test_pred = self.lm_model.predict(self.test_input).flatten()\n",
    "          val_pred = self.lm_model.predict(self.val_input).flatten()\n",
    "          vt_pred = self.lm_model.predict(self.vt_input).flatten()\n",
    "          sup_pred = self.lm_model.predict(self.sup_input).flatten()\n",
    "\n",
    "          # Calculando as metricas com a saida desnormalizada\n",
    "          pred_denorm = out_scaler.inverse_transform(pred.reshape(-1, 1))\n",
    "          test_pred_denorm = out_scaler.inverse_transform(test_pred.reshape(-1, 1))\n",
    "          val_pred_denorm = out_scaler.inverse_transform(val_pred.reshape(-1, 1))\n",
    "          vt_pred_denorm = out_scaler.inverse_transform(vt_pred.reshape(-1, 1))\n",
    "          sup_pred_denorm = out_scaler.inverse_transform(sup_pred.reshape(-1, 1))\n",
    "\n",
    "          out_denorm = out_scaler.inverse_transform(self._data[1].reshape(-1, 1))\n",
    "          test_denorm = out_scaler.inverse_transform(self._test[1].reshape(-1, 1))\n",
    "          val_denorm = out_scaler.inverse_transform(self._val[1].reshape(-1, 1))\n",
    "          vt_denorm = out_scaler.inverse_transform(self._vt[1].reshape(-1, 1))    \n",
    "          sup_denorm = out_scaler.inverse_transform(self._sup[1].reshape(-1, 1))\n",
    "\n",
    "          r2 = r2_score(out_denorm, pred_denorm)\n",
    "          r2_test = r2_score(test_denorm, test_pred_denorm)\n",
    "          r2_val = r2_score(val_denorm, val_pred_denorm)\n",
    "          r2_vt = r2_score(vt_denorm,  vt_pred_denorm)\n",
    "          r2_sup = r2_score(sup_denorm,  sup_pred_denorm)\n",
    "\n",
    "          mse = mean_squared_error(out_denorm, pred_denorm)\n",
    "          mse_test = mean_squared_error(test_denorm, test_pred_denorm)\n",
    "          mse_val = mean_squared_error(val_denorm, val_pred_denorm)\n",
    "          mse_vt = mean_squared_error(vt_denorm,  vt_pred_denorm)\n",
    "          mse_sup = mean_squared_error(sup_denorm,  sup_pred_denorm)\n",
    "          \n",
    "          mape, r2_adj, rsd, rmse, aic, bic = self.get_new_metrics(out_denorm, pred_denorm, r2, mse)\n",
    "          metrics = {\n",
    "                          'r2': r2,\n",
    "                          'r2_sup': r2_sup,\n",
    "                          'r2_test': r2_test,\n",
    "                          'r2_val': r2_val,\n",
    "                          'r2_vt': r2_vt,\n",
    "                          'mse': mse,\n",
    "                          'mse_sup': mse_sup,\n",
    "                          'mse_test': mse_test,\n",
    "                          'mse_val': mse_val,\n",
    "                          'mse_vt': mse_vt,\n",
    "                          'mape': mape,\n",
    "                          'rmse': rmse,\n",
    "                          'r2_adj': r2_adj,\n",
    "                          'rsd': rsd,\n",
    "                          'aic': aic,\n",
    "                          'bic': bic\n",
    "                          }\n",
    "\n",
    "          return metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "from itertools import product\n",
    "\n",
    "class Tester:\n",
    "  def __init__(self, _df, _df_1000,  run_times=500, dataset_run_times=10):\n",
    "    self.run_times = run_times\n",
    "    self.better_metrics = {}\n",
    "    self.dataset_run_times = dataset_run_times\n",
    "    self.input, self.output = split_df(_df)\n",
    "    self.input_1000, self.output_1000 = split_df(_df_1000)\n",
    "  \n",
    "  def setArchitecure(self, trainer, _hidden_sizes, _pg, _lr):\n",
    "    shuffler = ShuffleArchitecture(input_size=2,\n",
    "                                    hidden_sizes=_hidden_sizes,\n",
    "                                    output_size=1,\n",
    "                                    act_h='tanh',\n",
    "                                    act_o='linear',\n",
    "                                    param_reg=_pg)\n",
    "    shuffler.set_architecture()    \n",
    "    trainer.lm_model = shuffler.create_model(_lr)\n",
    "    trainer.k = shuffler.compute_k()\n",
    "\n",
    "  def Train(self, trainer, epochs=1000):\n",
    "    trainer.train_using_lm(trainer.train_dataset, epochs=epochs)\n",
    "    return(trainer.get_metrics(), trainer.lm_model)\n",
    "\n",
    "  def SaveModelWeights(self, model, fileName):\n",
    "    path = f\"../models/{fileName}.keras\"\n",
    "    open(path,'w').close()\n",
    "    model.save_weights(path)\n",
    "\n",
    "  def SaveDataset(self, trainer, fileName):\n",
    "    path = f\"../dataset/{fileName}.pkl\" \n",
    "    with open(path, 'wb') as f:\n",
    "      pickle.dump((trainer._data, trainer._train, trainer._vt, trainer._val, trainer._test), f)\n",
    "      \n",
    "  def LoopWeights(self, sort_by, boundarie, trainer, idx):\n",
    "    better_model = 0\n",
    "    save = False\n",
    "\n",
    "    for i in range(self.run_times):\n",
    "      print (f\"+++++++++++ [{idx}] | {i + 1} ++++++++++++++++++\")\n",
    "      metrics, model = self.Train(trainer)\n",
    "      if (metrics[sort_by] <= boundarie): # should be >= to acsending metrics\n",
    "        fileName = f\"model_{idx}_{better_model}\"\n",
    "        self.SaveModelWeights(model, fileName)\n",
    "        self.better_metrics[fileName] = metrics\n",
    "        better_model += 1\n",
    "        save = True\n",
    "    return(save)\n",
    "\n",
    "  def Loop(self, sort_by, boundarie, hidden_sizes, regularizers, learning_rate):\n",
    "    trainer = TrainWithSmallDataset()\n",
    "\n",
    "    for count, (hidden_size, reg, lr) in enumerate(product(hidden_sizes, regularizers, learning_rate), start=1):\n",
    "      header =  f\"Hidden Size={hidden_size}, regularizer={reg}, learning_rate={lr}\"\n",
    "      print(f\"Testando combinacao{count}: {header}\")\n",
    "      self.setArchitecure(trainer, hidden_size, reg, lr)\n",
    "      for j in range(self.dataset_run_times):\n",
    "        trainer.split_dataset(self.input, self.output, self.input_1000, self.output_1000)\n",
    "        if (self.LoopWeights(sort_by, boundarie, trainer, f\"{count}_{j}\") == True):\n",
    "          self.SaveDataset(trainer, f\"dataset_{count}_{j}\")\n",
    "          self.DisplayBetterResults('mse_sup', header, f\"{count}_{j}\")\n",
    "        self.better_metrics = {}\n",
    "\n",
    "  def DisplayBetterResults(self, sort_by, header, dataset=0):\n",
    "    df = pd.DataFrame.from_dict(self.better_metrics, orient='index')\n",
    "    df = df.sort_values([sort_by])\n",
    "    display(df)\n",
    "    path = f'../results/metrics_{dataset}'\n",
    "    df.to_excel(f\"{path}.xlsx\", index=True)\n",
    "    print(f\"DataFrame salvo em {path}\")\n",
    "    with open(f\"{path}.txt\", 'w') as arquivo:\n",
    "      arquivo.write(header)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Treinando apenas com dados originais\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testando combinacao1: Hidden Size=[2, 12], regularizer=0.02, learning_rate=0.01\n",
      "+++++++++++ [1_0] | 1 ++++++++++++++++++\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stopped at epoch:  11\n",
      "4/4 [==============================] - 0s 2ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "32/32 [==============================] - 0s 2ms/step\n",
      "+++++++++++ [1_0] | 2 ++++++++++++++++++\n",
      "Stopped at epoch:  7\n",
      "4/4 [==============================] - 0s 2ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "32/32 [==============================] - 0s 2ms/step\n",
      "+++++++++++ [1_0] | 3 ++++++++++++++++++\n",
      "Stopped at epoch:  9\n",
      "4/4 [==============================] - 0s 3ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "32/32 [==============================] - 0s 2ms/step\n",
      "+++++++++++ [1_0] | 4 ++++++++++++++++++\n",
      "Stopped at epoch:  6\n",
      "4/4 [==============================] - 0s 2ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "32/32 [==============================] - 0s 2ms/step\n",
      "+++++++++++ [1_0] | 5 ++++++++++++++++++\n",
      "Stopped at epoch:  6\n",
      "4/4 [==============================] - 0s 2ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "32/32 [==============================] - 0s 2ms/step\n",
      "+++++++++++ [1_0] | 6 ++++++++++++++++++\n",
      "Stopped at epoch:  6\n",
      "4/4 [==============================] - 0s 2ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "32/32 [==============================] - 0s 2ms/step\n",
      "+++++++++++ [1_0] | 7 ++++++++++++++++++\n",
      "Stopped at epoch:  6\n",
      "4/4 [==============================] - 0s 2ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "32/32 [==============================] - 0s 2ms/step\n",
      "+++++++++++ [1_0] | 8 ++++++++++++++++++\n",
      "Stopped at epoch:  6\n",
      "4/4 [==============================] - 0s 2ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "32/32 [==============================] - 0s 2ms/step\n",
      "+++++++++++ [1_0] | 9 ++++++++++++++++++\n",
      "Stopped at epoch:  6\n",
      "4/4 [==============================] - 0s 2ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "32/32 [==============================] - 0s 2ms/step\n",
      "+++++++++++ [1_0] | 10 ++++++++++++++++++\n",
      "Stopped at epoch:  6\n",
      "4/4 [==============================] - 0s 2ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "32/32 [==============================] - 0s 2ms/step\n",
      "+++++++++++ [1_0] | 11 ++++++++++++++++++\n",
      "Stopped at epoch:  6\n",
      "4/4 [==============================] - 0s 2ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "32/32 [==============================] - 0s 2ms/step\n",
      "+++++++++++ [1_0] | 12 ++++++++++++++++++\n",
      "Stopped at epoch:  6\n",
      "4/4 [==============================] - 0s 2ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "32/32 [==============================] - 0s 2ms/step\n",
      "+++++++++++ [1_0] | 13 ++++++++++++++++++\n",
      "Stopped at epoch:  6\n",
      "4/4 [==============================] - 0s 2ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "32/32 [==============================] - 0s 2ms/step\n",
      "+++++++++++ [1_0] | 14 ++++++++++++++++++\n",
      "Stopped at epoch:  6\n",
      "4/4 [==============================] - 0s 2ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "32/32 [==============================] - 0s 2ms/step\n",
      "+++++++++++ [1_0] | 15 ++++++++++++++++++\n",
      "Stopped at epoch:  6\n",
      "4/4 [==============================] - 0s 3ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "32/32 [==============================] - 0s 2ms/step\n",
      "+++++++++++ [1_0] | 16 ++++++++++++++++++\n",
      "Stopped at epoch:  6\n",
      "4/4 [==============================] - 0s 2ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "32/32 [==============================] - 0s 2ms/step\n",
      "+++++++++++ [1_0] | 17 ++++++++++++++++++\n",
      "Stopped at epoch:  6\n",
      "4/4 [==============================] - 0s 2ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "32/32 [==============================] - 0s 2ms/step\n",
      "+++++++++++ [1_0] | 18 ++++++++++++++++++\n",
      "Stopped at epoch:  6\n",
      "4/4 [==============================] - 0s 2ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "32/32 [==============================] - 0s 2ms/step\n",
      "+++++++++++ [1_0] | 19 ++++++++++++++++++\n",
      "Stopped at epoch:  6\n",
      "4/4 [==============================] - 0s 2ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "32/32 [==============================] - 0s 2ms/step\n",
      "+++++++++++ [1_0] | 20 ++++++++++++++++++\n",
      "Stopped at epoch:  6\n",
      "4/4 [==============================] - 0s 2ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "32/32 [==============================] - 0s 2ms/step\n",
      "+++++++++++ [1_0] | 21 ++++++++++++++++++\n",
      "Stopped at epoch:  6\n",
      "4/4 [==============================] - 0s 2ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "32/32 [==============================] - 0s 2ms/step\n",
      "+++++++++++ [1_0] | 22 ++++++++++++++++++\n",
      "Stopped at epoch:  6\n",
      "4/4 [==============================] - 0s 3ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "+++++++++++ [1_0] | 23 ++++++++++++++++++\n",
      "Stopped at epoch:  6\n",
      "4/4 [==============================] - 0s 2ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "32/32 [==============================] - 0s 2ms/step\n",
      "+++++++++++ [1_0] | 24 ++++++++++++++++++\n",
      "Stopped at epoch:  7\n",
      "4/4 [==============================] - 0s 2ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "32/32 [==============================] - 0s 2ms/step\n",
      "+++++++++++ [1_0] | 25 ++++++++++++++++++\n",
      "Stopped at epoch:  7\n",
      "4/4 [==============================] - 0s 2ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "32/32 [==============================] - 0s 2ms/step\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>r2</th>\n",
       "      <th>r2_sup</th>\n",
       "      <th>r2_test</th>\n",
       "      <th>r2_val</th>\n",
       "      <th>r2_vt</th>\n",
       "      <th>mse</th>\n",
       "      <th>mse_sup</th>\n",
       "      <th>mse_test</th>\n",
       "      <th>mse_val</th>\n",
       "      <th>mse_vt</th>\n",
       "      <th>mape</th>\n",
       "      <th>rmse</th>\n",
       "      <th>r2_adj</th>\n",
       "      <th>rsd</th>\n",
       "      <th>aic</th>\n",
       "      <th>bic</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>model_1_0_24</th>\n",
       "      <td>0.860298</td>\n",
       "      <td>0.508708</td>\n",
       "      <td>0.886306</td>\n",
       "      <td>0.861026</td>\n",
       "      <td>0.873576</td>\n",
       "      <td>0.051394</td>\n",
       "      <td>0.291652</td>\n",
       "      <td>0.051633</td>\n",
       "      <td>0.067203</td>\n",
       "      <td>0.059418</td>\n",
       "      <td>0.117060</td>\n",
       "      <td>0.226703</td>\n",
       "      <td>0.750533</td>\n",
       "      <td>0.228524</td>\n",
       "      <td>115.936465</td>\n",
       "      <td>271.931970</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>model_1_0_23</th>\n",
       "      <td>0.845070</td>\n",
       "      <td>0.497537</td>\n",
       "      <td>0.870558</td>\n",
       "      <td>0.849722</td>\n",
       "      <td>0.860152</td>\n",
       "      <td>0.056996</td>\n",
       "      <td>0.298284</td>\n",
       "      <td>0.058785</td>\n",
       "      <td>0.072669</td>\n",
       "      <td>0.065727</td>\n",
       "      <td>0.123794</td>\n",
       "      <td>0.238739</td>\n",
       "      <td>0.723340</td>\n",
       "      <td>0.240656</td>\n",
       "      <td>115.729541</td>\n",
       "      <td>271.725046</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>model_1_0_22</th>\n",
       "      <td>0.841759</td>\n",
       "      <td>0.477930</td>\n",
       "      <td>0.850430</td>\n",
       "      <td>0.862069</td>\n",
       "      <td>0.856779</td>\n",
       "      <td>0.058214</td>\n",
       "      <td>0.309923</td>\n",
       "      <td>0.067925</td>\n",
       "      <td>0.066698</td>\n",
       "      <td>0.067312</td>\n",
       "      <td>0.130657</td>\n",
       "      <td>0.241276</td>\n",
       "      <td>0.717427</td>\n",
       "      <td>0.243214</td>\n",
       "      <td>115.687250</td>\n",
       "      <td>271.682755</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>model_1_0_21</th>\n",
       "      <td>0.841580</td>\n",
       "      <td>0.476108</td>\n",
       "      <td>0.850352</td>\n",
       "      <td>0.867060</td>\n",
       "      <td>0.859309</td>\n",
       "      <td>0.058280</td>\n",
       "      <td>0.311005</td>\n",
       "      <td>0.067961</td>\n",
       "      <td>0.064285</td>\n",
       "      <td>0.066123</td>\n",
       "      <td>0.131303</td>\n",
       "      <td>0.241413</td>\n",
       "      <td>0.717108</td>\n",
       "      <td>0.243352</td>\n",
       "      <td>115.684990</td>\n",
       "      <td>271.680494</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>model_1_0_20</th>\n",
       "      <td>0.825873</td>\n",
       "      <td>0.469535</td>\n",
       "      <td>0.830066</td>\n",
       "      <td>0.853106</td>\n",
       "      <td>0.842330</td>\n",
       "      <td>0.064059</td>\n",
       "      <td>0.314907</td>\n",
       "      <td>0.077174</td>\n",
       "      <td>0.071032</td>\n",
       "      <td>0.074103</td>\n",
       "      <td>0.136528</td>\n",
       "      <td>0.253098</td>\n",
       "      <td>0.689059</td>\n",
       "      <td>0.255131</td>\n",
       "      <td>115.495913</td>\n",
       "      <td>271.491418</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>model_1_0_19</th>\n",
       "      <td>0.808859</td>\n",
       "      <td>0.461237</td>\n",
       "      <td>0.813730</td>\n",
       "      <td>0.834689</td>\n",
       "      <td>0.824963</td>\n",
       "      <td>0.070318</td>\n",
       "      <td>0.319833</td>\n",
       "      <td>0.084592</td>\n",
       "      <td>0.079938</td>\n",
       "      <td>0.082265</td>\n",
       "      <td>0.142300</td>\n",
       "      <td>0.265175</td>\n",
       "      <td>0.658677</td>\n",
       "      <td>0.267305</td>\n",
       "      <td>115.309459</td>\n",
       "      <td>271.304964</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>model_1_0_18</th>\n",
       "      <td>0.808218</td>\n",
       "      <td>0.460826</td>\n",
       "      <td>0.811294</td>\n",
       "      <td>0.834467</td>\n",
       "      <td>0.823672</td>\n",
       "      <td>0.070554</td>\n",
       "      <td>0.320077</td>\n",
       "      <td>0.085699</td>\n",
       "      <td>0.080045</td>\n",
       "      <td>0.082872</td>\n",
       "      <td>0.142232</td>\n",
       "      <td>0.265619</td>\n",
       "      <td>0.657532</td>\n",
       "      <td>0.267753</td>\n",
       "      <td>115.302765</td>\n",
       "      <td>271.298270</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>model_1_0_17</th>\n",
       "      <td>0.805692</td>\n",
       "      <td>0.459758</td>\n",
       "      <td>0.809398</td>\n",
       "      <td>0.830737</td>\n",
       "      <td>0.820836</td>\n",
       "      <td>0.071483</td>\n",
       "      <td>0.320711</td>\n",
       "      <td>0.086560</td>\n",
       "      <td>0.081849</td>\n",
       "      <td>0.084205</td>\n",
       "      <td>0.143067</td>\n",
       "      <td>0.267363</td>\n",
       "      <td>0.653021</td>\n",
       "      <td>0.269510</td>\n",
       "      <td>115.276593</td>\n",
       "      <td>271.272098</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>model_1_0_16</th>\n",
       "      <td>0.802342</td>\n",
       "      <td>0.455643</td>\n",
       "      <td>0.805480</td>\n",
       "      <td>0.830391</td>\n",
       "      <td>0.818766</td>\n",
       "      <td>0.072715</td>\n",
       "      <td>0.323154</td>\n",
       "      <td>0.088339</td>\n",
       "      <td>0.082016</td>\n",
       "      <td>0.085178</td>\n",
       "      <td>0.144637</td>\n",
       "      <td>0.269658</td>\n",
       "      <td>0.647039</td>\n",
       "      <td>0.271824</td>\n",
       "      <td>115.242405</td>\n",
       "      <td>271.237910</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>model_1_0_14</th>\n",
       "      <td>0.796998</td>\n",
       "      <td>0.452635</td>\n",
       "      <td>0.800480</td>\n",
       "      <td>0.823857</td>\n",
       "      <td>0.812989</td>\n",
       "      <td>0.074681</td>\n",
       "      <td>0.324940</td>\n",
       "      <td>0.090610</td>\n",
       "      <td>0.085176</td>\n",
       "      <td>0.087893</td>\n",
       "      <td>0.146353</td>\n",
       "      <td>0.273278</td>\n",
       "      <td>0.637497</td>\n",
       "      <td>0.275473</td>\n",
       "      <td>115.189056</td>\n",
       "      <td>271.184561</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>model_1_0_15</th>\n",
       "      <td>0.798882</td>\n",
       "      <td>0.452589</td>\n",
       "      <td>0.801959</td>\n",
       "      <td>0.827586</td>\n",
       "      <td>0.815622</td>\n",
       "      <td>0.073988</td>\n",
       "      <td>0.324967</td>\n",
       "      <td>0.089938</td>\n",
       "      <td>0.083373</td>\n",
       "      <td>0.086655</td>\n",
       "      <td>0.146017</td>\n",
       "      <td>0.272008</td>\n",
       "      <td>0.640861</td>\n",
       "      <td>0.274193</td>\n",
       "      <td>115.207700</td>\n",
       "      <td>271.203204</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>model_1_0_13</th>\n",
       "      <td>0.769494</td>\n",
       "      <td>0.437128</td>\n",
       "      <td>0.773722</td>\n",
       "      <td>0.793883</td>\n",
       "      <td>0.784641</td>\n",
       "      <td>0.084800</td>\n",
       "      <td>0.334145</td>\n",
       "      <td>0.102761</td>\n",
       "      <td>0.099670</td>\n",
       "      <td>0.101216</td>\n",
       "      <td>0.155028</td>\n",
       "      <td>0.291204</td>\n",
       "      <td>0.588382</td>\n",
       "      <td>0.293543</td>\n",
       "      <td>114.934930</td>\n",
       "      <td>270.930435</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>model_1_0_12</th>\n",
       "      <td>0.737630</td>\n",
       "      <td>0.416949</td>\n",
       "      <td>0.743103</td>\n",
       "      <td>0.766460</td>\n",
       "      <td>0.755740</td>\n",
       "      <td>0.096522</td>\n",
       "      <td>0.346124</td>\n",
       "      <td>0.116667</td>\n",
       "      <td>0.112931</td>\n",
       "      <td>0.114799</td>\n",
       "      <td>0.165242</td>\n",
       "      <td>0.310680</td>\n",
       "      <td>0.531481</td>\n",
       "      <td>0.313175</td>\n",
       "      <td>114.675969</td>\n",
       "      <td>270.671474</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>model_1_0_11</th>\n",
       "      <td>0.706622</td>\n",
       "      <td>0.408281</td>\n",
       "      <td>0.736798</td>\n",
       "      <td>0.725697</td>\n",
       "      <td>0.731724</td>\n",
       "      <td>0.107929</td>\n",
       "      <td>0.351270</td>\n",
       "      <td>0.119530</td>\n",
       "      <td>0.132642</td>\n",
       "      <td>0.126086</td>\n",
       "      <td>0.170488</td>\n",
       "      <td>0.328526</td>\n",
       "      <td>0.476111</td>\n",
       "      <td>0.331164</td>\n",
       "      <td>114.452563</td>\n",
       "      <td>270.448068</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>model_1_0_10</th>\n",
       "      <td>0.685840</td>\n",
       "      <td>0.378972</td>\n",
       "      <td>0.712117</td>\n",
       "      <td>0.734024</td>\n",
       "      <td>0.724083</td>\n",
       "      <td>0.115575</td>\n",
       "      <td>0.368669</td>\n",
       "      <td>0.130739</td>\n",
       "      <td>0.128616</td>\n",
       "      <td>0.129677</td>\n",
       "      <td>0.183100</td>\n",
       "      <td>0.339963</td>\n",
       "      <td>0.439000</td>\n",
       "      <td>0.342693</td>\n",
       "      <td>114.315679</td>\n",
       "      <td>270.311184</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>model_1_0_9</th>\n",
       "      <td>0.654363</td>\n",
       "      <td>0.357667</td>\n",
       "      <td>0.682677</td>\n",
       "      <td>0.700626</td>\n",
       "      <td>0.692678</td>\n",
       "      <td>0.127154</td>\n",
       "      <td>0.381316</td>\n",
       "      <td>0.144108</td>\n",
       "      <td>0.144766</td>\n",
       "      <td>0.144437</td>\n",
       "      <td>0.191853</td>\n",
       "      <td>0.356587</td>\n",
       "      <td>0.382791</td>\n",
       "      <td>0.359451</td>\n",
       "      <td>114.124705</td>\n",
       "      <td>270.120210</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>model_1_0_8</th>\n",
       "      <td>0.574640</td>\n",
       "      <td>0.266045</td>\n",
       "      <td>0.614455</td>\n",
       "      <td>0.626889</td>\n",
       "      <td>0.621784</td>\n",
       "      <td>0.156483</td>\n",
       "      <td>0.435707</td>\n",
       "      <td>0.175090</td>\n",
       "      <td>0.180422</td>\n",
       "      <td>0.177756</td>\n",
       "      <td>0.221568</td>\n",
       "      <td>0.395580</td>\n",
       "      <td>0.240429</td>\n",
       "      <td>0.398757</td>\n",
       "      <td>113.709614</td>\n",
       "      <td>269.705119</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>model_1_0_7</th>\n",
       "      <td>0.527309</td>\n",
       "      <td>0.251549</td>\n",
       "      <td>0.580075</td>\n",
       "      <td>0.545069</td>\n",
       "      <td>0.563082</td>\n",
       "      <td>0.173896</td>\n",
       "      <td>0.444313</td>\n",
       "      <td>0.190704</td>\n",
       "      <td>0.219987</td>\n",
       "      <td>0.205345</td>\n",
       "      <td>0.226580</td>\n",
       "      <td>0.417008</td>\n",
       "      <td>0.155909</td>\n",
       "      <td>0.420357</td>\n",
       "      <td>113.498600</td>\n",
       "      <td>269.494105</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>model_1_0_6</th>\n",
       "      <td>0.459530</td>\n",
       "      <td>0.217671</td>\n",
       "      <td>0.521257</td>\n",
       "      <td>0.444193</td>\n",
       "      <td>0.482770</td>\n",
       "      <td>0.198831</td>\n",
       "      <td>0.464424</td>\n",
       "      <td>0.217416</td>\n",
       "      <td>0.268767</td>\n",
       "      <td>0.243091</td>\n",
       "      <td>0.237172</td>\n",
       "      <td>0.445904</td>\n",
       "      <td>0.034874</td>\n",
       "      <td>0.449486</td>\n",
       "      <td>113.230604</td>\n",
       "      <td>269.226109</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>model_1_0_5</th>\n",
       "      <td>0.416922</td>\n",
       "      <td>0.194365</td>\n",
       "      <td>0.481048</td>\n",
       "      <td>0.391387</td>\n",
       "      <td>0.436178</td>\n",
       "      <td>0.214505</td>\n",
       "      <td>0.478260</td>\n",
       "      <td>0.235676</td>\n",
       "      <td>0.294301</td>\n",
       "      <td>0.264989</td>\n",
       "      <td>0.241473</td>\n",
       "      <td>0.463147</td>\n",
       "      <td>-0.041210</td>\n",
       "      <td>0.466867</td>\n",
       "      <td>113.078843</td>\n",
       "      <td>269.074347</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>model_1_0_4</th>\n",
       "      <td>0.300424</td>\n",
       "      <td>0.151148</td>\n",
       "      <td>0.383509</td>\n",
       "      <td>0.234292</td>\n",
       "      <td>0.308237</td>\n",
       "      <td>0.257363</td>\n",
       "      <td>0.503915</td>\n",
       "      <td>0.279972</td>\n",
       "      <td>0.370267</td>\n",
       "      <td>0.325119</td>\n",
       "      <td>0.257092</td>\n",
       "      <td>0.507310</td>\n",
       "      <td>-0.249244</td>\n",
       "      <td>0.511385</td>\n",
       "      <td>112.714534</td>\n",
       "      <td>268.710038</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>model_1_0_3</th>\n",
       "      <td>0.201020</td>\n",
       "      <td>0.087222</td>\n",
       "      <td>0.277903</td>\n",
       "      <td>0.136562</td>\n",
       "      <td>0.206938</td>\n",
       "      <td>0.293932</td>\n",
       "      <td>0.541865</td>\n",
       "      <td>0.327932</td>\n",
       "      <td>0.417525</td>\n",
       "      <td>0.372729</td>\n",
       "      <td>0.275828</td>\n",
       "      <td>0.542155</td>\n",
       "      <td>-0.426750</td>\n",
       "      <td>0.546510</td>\n",
       "      <td>112.448813</td>\n",
       "      <td>268.444317</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>model_1_0_2</th>\n",
       "      <td>0.061585</td>\n",
       "      <td>-0.010713</td>\n",
       "      <td>0.160180</td>\n",
       "      <td>0.023819</td>\n",
       "      <td>0.092062</td>\n",
       "      <td>0.345228</td>\n",
       "      <td>0.600003</td>\n",
       "      <td>0.381394</td>\n",
       "      <td>0.472043</td>\n",
       "      <td>0.426719</td>\n",
       "      <td>0.300562</td>\n",
       "      <td>0.587561</td>\n",
       "      <td>-0.675741</td>\n",
       "      <td>0.592281</td>\n",
       "      <td>112.127100</td>\n",
       "      <td>268.122604</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>model_1_0_1</th>\n",
       "      <td>-0.059892</td>\n",
       "      <td>-0.077683</td>\n",
       "      <td>-0.047765</td>\n",
       "      <td>-0.029039</td>\n",
       "      <td>-0.035597</td>\n",
       "      <td>0.389918</td>\n",
       "      <td>0.639759</td>\n",
       "      <td>0.475830</td>\n",
       "      <td>0.497603</td>\n",
       "      <td>0.486717</td>\n",
       "      <td>0.356615</td>\n",
       "      <td>0.624434</td>\n",
       "      <td>-0.892665</td>\n",
       "      <td>0.629450</td>\n",
       "      <td>111.883639</td>\n",
       "      <td>267.879144</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>model_1_0_0</th>\n",
       "      <td>-0.085782</td>\n",
       "      <td>-0.080708</td>\n",
       "      <td>-0.091315</td>\n",
       "      <td>-0.072768</td>\n",
       "      <td>-0.079133</td>\n",
       "      <td>0.399442</td>\n",
       "      <td>0.641555</td>\n",
       "      <td>0.495608</td>\n",
       "      <td>0.518749</td>\n",
       "      <td>0.507178</td>\n",
       "      <td>0.358717</td>\n",
       "      <td>0.632014</td>\n",
       "      <td>-0.938896</td>\n",
       "      <td>0.637091</td>\n",
       "      <td>111.835373</td>\n",
       "      <td>267.830878</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    r2    r2_sup   r2_test    r2_val     r2_vt       mse  \\\n",
       "model_1_0_24  0.860298  0.508708  0.886306  0.861026  0.873576  0.051394   \n",
       "model_1_0_23  0.845070  0.497537  0.870558  0.849722  0.860152  0.056996   \n",
       "model_1_0_22  0.841759  0.477930  0.850430  0.862069  0.856779  0.058214   \n",
       "model_1_0_21  0.841580  0.476108  0.850352  0.867060  0.859309  0.058280   \n",
       "model_1_0_20  0.825873  0.469535  0.830066  0.853106  0.842330  0.064059   \n",
       "model_1_0_19  0.808859  0.461237  0.813730  0.834689  0.824963  0.070318   \n",
       "model_1_0_18  0.808218  0.460826  0.811294  0.834467  0.823672  0.070554   \n",
       "model_1_0_17  0.805692  0.459758  0.809398  0.830737  0.820836  0.071483   \n",
       "model_1_0_16  0.802342  0.455643  0.805480  0.830391  0.818766  0.072715   \n",
       "model_1_0_14  0.796998  0.452635  0.800480  0.823857  0.812989  0.074681   \n",
       "model_1_0_15  0.798882  0.452589  0.801959  0.827586  0.815622  0.073988   \n",
       "model_1_0_13  0.769494  0.437128  0.773722  0.793883  0.784641  0.084800   \n",
       "model_1_0_12  0.737630  0.416949  0.743103  0.766460  0.755740  0.096522   \n",
       "model_1_0_11  0.706622  0.408281  0.736798  0.725697  0.731724  0.107929   \n",
       "model_1_0_10  0.685840  0.378972  0.712117  0.734024  0.724083  0.115575   \n",
       "model_1_0_9   0.654363  0.357667  0.682677  0.700626  0.692678  0.127154   \n",
       "model_1_0_8   0.574640  0.266045  0.614455  0.626889  0.621784  0.156483   \n",
       "model_1_0_7   0.527309  0.251549  0.580075  0.545069  0.563082  0.173896   \n",
       "model_1_0_6   0.459530  0.217671  0.521257  0.444193  0.482770  0.198831   \n",
       "model_1_0_5   0.416922  0.194365  0.481048  0.391387  0.436178  0.214505   \n",
       "model_1_0_4   0.300424  0.151148  0.383509  0.234292  0.308237  0.257363   \n",
       "model_1_0_3   0.201020  0.087222  0.277903  0.136562  0.206938  0.293932   \n",
       "model_1_0_2   0.061585 -0.010713  0.160180  0.023819  0.092062  0.345228   \n",
       "model_1_0_1  -0.059892 -0.077683 -0.047765 -0.029039 -0.035597  0.389918   \n",
       "model_1_0_0  -0.085782 -0.080708 -0.091315 -0.072768 -0.079133  0.399442   \n",
       "\n",
       "               mse_sup  mse_test   mse_val    mse_vt      mape      rmse  \\\n",
       "model_1_0_24  0.291652  0.051633  0.067203  0.059418  0.117060  0.226703   \n",
       "model_1_0_23  0.298284  0.058785  0.072669  0.065727  0.123794  0.238739   \n",
       "model_1_0_22  0.309923  0.067925  0.066698  0.067312  0.130657  0.241276   \n",
       "model_1_0_21  0.311005  0.067961  0.064285  0.066123  0.131303  0.241413   \n",
       "model_1_0_20  0.314907  0.077174  0.071032  0.074103  0.136528  0.253098   \n",
       "model_1_0_19  0.319833  0.084592  0.079938  0.082265  0.142300  0.265175   \n",
       "model_1_0_18  0.320077  0.085699  0.080045  0.082872  0.142232  0.265619   \n",
       "model_1_0_17  0.320711  0.086560  0.081849  0.084205  0.143067  0.267363   \n",
       "model_1_0_16  0.323154  0.088339  0.082016  0.085178  0.144637  0.269658   \n",
       "model_1_0_14  0.324940  0.090610  0.085176  0.087893  0.146353  0.273278   \n",
       "model_1_0_15  0.324967  0.089938  0.083373  0.086655  0.146017  0.272008   \n",
       "model_1_0_13  0.334145  0.102761  0.099670  0.101216  0.155028  0.291204   \n",
       "model_1_0_12  0.346124  0.116667  0.112931  0.114799  0.165242  0.310680   \n",
       "model_1_0_11  0.351270  0.119530  0.132642  0.126086  0.170488  0.328526   \n",
       "model_1_0_10  0.368669  0.130739  0.128616  0.129677  0.183100  0.339963   \n",
       "model_1_0_9   0.381316  0.144108  0.144766  0.144437  0.191853  0.356587   \n",
       "model_1_0_8   0.435707  0.175090  0.180422  0.177756  0.221568  0.395580   \n",
       "model_1_0_7   0.444313  0.190704  0.219987  0.205345  0.226580  0.417008   \n",
       "model_1_0_6   0.464424  0.217416  0.268767  0.243091  0.237172  0.445904   \n",
       "model_1_0_5   0.478260  0.235676  0.294301  0.264989  0.241473  0.463147   \n",
       "model_1_0_4   0.503915  0.279972  0.370267  0.325119  0.257092  0.507310   \n",
       "model_1_0_3   0.541865  0.327932  0.417525  0.372729  0.275828  0.542155   \n",
       "model_1_0_2   0.600003  0.381394  0.472043  0.426719  0.300562  0.587561   \n",
       "model_1_0_1   0.639759  0.475830  0.497603  0.486717  0.356615  0.624434   \n",
       "model_1_0_0   0.641555  0.495608  0.518749  0.507178  0.358717  0.632014   \n",
       "\n",
       "                r2_adj       rsd         aic         bic  \n",
       "model_1_0_24  0.750533  0.228524  115.936465  271.931970  \n",
       "model_1_0_23  0.723340  0.240656  115.729541  271.725046  \n",
       "model_1_0_22  0.717427  0.243214  115.687250  271.682755  \n",
       "model_1_0_21  0.717108  0.243352  115.684990  271.680494  \n",
       "model_1_0_20  0.689059  0.255131  115.495913  271.491418  \n",
       "model_1_0_19  0.658677  0.267305  115.309459  271.304964  \n",
       "model_1_0_18  0.657532  0.267753  115.302765  271.298270  \n",
       "model_1_0_17  0.653021  0.269510  115.276593  271.272098  \n",
       "model_1_0_16  0.647039  0.271824  115.242405  271.237910  \n",
       "model_1_0_14  0.637497  0.275473  115.189056  271.184561  \n",
       "model_1_0_15  0.640861  0.274193  115.207700  271.203204  \n",
       "model_1_0_13  0.588382  0.293543  114.934930  270.930435  \n",
       "model_1_0_12  0.531481  0.313175  114.675969  270.671474  \n",
       "model_1_0_11  0.476111  0.331164  114.452563  270.448068  \n",
       "model_1_0_10  0.439000  0.342693  114.315679  270.311184  \n",
       "model_1_0_9   0.382791  0.359451  114.124705  270.120210  \n",
       "model_1_0_8   0.240429  0.398757  113.709614  269.705119  \n",
       "model_1_0_7   0.155909  0.420357  113.498600  269.494105  \n",
       "model_1_0_6   0.034874  0.449486  113.230604  269.226109  \n",
       "model_1_0_5  -0.041210  0.466867  113.078843  269.074347  \n",
       "model_1_0_4  -0.249244  0.511385  112.714534  268.710038  \n",
       "model_1_0_3  -0.426750  0.546510  112.448813  268.444317  \n",
       "model_1_0_2  -0.675741  0.592281  112.127100  268.122604  \n",
       "model_1_0_1  -0.892665  0.629450  111.883639  267.879144  \n",
       "model_1_0_0  -0.938896  0.637091  111.835373  267.830878  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataFrame salvo em ../results/metrics_1_0\n",
      "+++++++++++ [1_1] | 1 ++++++++++++++++++\n",
      "Stopped at epoch:  8\n",
      "4/4 [==============================] - 0s 2ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "32/32 [==============================] - 0s 2ms/step\n",
      "+++++++++++ [1_1] | 2 ++++++++++++++++++\n",
      "Stopped at epoch:  6\n",
      "4/4 [==============================] - 0s 2ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "32/32 [==============================] - 0s 2ms/step\n",
      "+++++++++++ [1_1] | 3 ++++++++++++++++++\n",
      "Stopped at epoch:  7\n",
      "4/4 [==============================] - 0s 3ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "+++++++++++ [1_1] | 4 ++++++++++++++++++\n",
      "Stopped at epoch:  7\n",
      "4/4 [==============================] - 0s 2ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "+++++++++++ [1_1] | 5 ++++++++++++++++++\n",
      "Stopped at epoch:  14\n",
      "4/4 [==============================] - 0s 2ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "32/32 [==============================] - 0s 2ms/step\n",
      "+++++++++++ [1_1] | 6 ++++++++++++++++++\n",
      "Stopped at epoch:  6\n",
      "4/4 [==============================] - 0s 2ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "32/32 [==============================] - 0s 2ms/step\n",
      "+++++++++++ [1_1] | 7 ++++++++++++++++++\n",
      "Stopped at epoch:  6\n",
      "4/4 [==============================] - 0s 2ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "32/32 [==============================] - 0s 2ms/step\n",
      "+++++++++++ [1_1] | 8 ++++++++++++++++++\n",
      "Stopped at epoch:  6\n",
      "4/4 [==============================] - 0s 2ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "32/32 [==============================] - 0s 2ms/step\n",
      "+++++++++++ [1_1] | 9 ++++++++++++++++++\n",
      "Stopped at epoch:  6\n",
      "4/4 [==============================] - 0s 2ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "32/32 [==============================] - 0s 2ms/step\n",
      "+++++++++++ [1_1] | 10 ++++++++++++++++++\n",
      "Stopped at epoch:  6\n",
      "4/4 [==============================] - 0s 2ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "32/32 [==============================] - 0s 2ms/step\n",
      "+++++++++++ [1_1] | 11 ++++++++++++++++++\n",
      "Stopped at epoch:  6\n",
      "4/4 [==============================] - 0s 2ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "32/32 [==============================] - 0s 2ms/step\n",
      "+++++++++++ [1_1] | 12 ++++++++++++++++++\n",
      "Stopped at epoch:  6\n",
      "4/4 [==============================] - 0s 2ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "32/32 [==============================] - 0s 2ms/step\n",
      "+++++++++++ [1_1] | 13 ++++++++++++++++++\n",
      "Stopped at epoch:  6\n",
      "4/4 [==============================] - 0s 2ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "32/32 [==============================] - 0s 2ms/step\n",
      "+++++++++++ [1_1] | 14 ++++++++++++++++++\n",
      "Stopped at epoch:  6\n",
      "4/4 [==============================] - 0s 2ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "2/2 [==============================] - 0s 6ms/step\n",
      "32/32 [==============================] - 0s 2ms/step\n",
      "+++++++++++ [1_1] | 15 ++++++++++++++++++\n",
      "Stopped at epoch:  6\n",
      "4/4 [==============================] - 0s 2ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "32/32 [==============================] - 0s 2ms/step\n",
      "+++++++++++ [1_1] | 16 ++++++++++++++++++\n",
      "Stopped at epoch:  6\n",
      "4/4 [==============================] - 0s 2ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "32/32 [==============================] - 0s 2ms/step\n",
      "+++++++++++ [1_1] | 17 ++++++++++++++++++\n",
      "Stopped at epoch:  6\n",
      "4/4 [==============================] - 0s 3ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "32/32 [==============================] - 0s 2ms/step\n",
      "+++++++++++ [1_1] | 18 ++++++++++++++++++\n",
      "Stopped at epoch:  6\n",
      "4/4 [==============================] - 0s 2ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "32/32 [==============================] - 0s 2ms/step\n",
      "+++++++++++ [1_1] | 19 ++++++++++++++++++\n",
      "Stopped at epoch:  6\n",
      "4/4 [==============================] - 0s 2ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "32/32 [==============================] - 0s 2ms/step\n",
      "+++++++++++ [1_1] | 20 ++++++++++++++++++\n",
      "Stopped at epoch:  6\n",
      "4/4 [==============================] - 0s 2ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "32/32 [==============================] - 0s 2ms/step\n",
      "+++++++++++ [1_1] | 21 ++++++++++++++++++\n",
      "Stopped at epoch:  6\n",
      "4/4 [==============================] - 0s 2ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "+++++++++++ [1_1] | 22 ++++++++++++++++++\n",
      "Stopped at epoch:  6\n",
      "4/4 [==============================] - 0s 3ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "32/32 [==============================] - 0s 2ms/step\n",
      "+++++++++++ [1_1] | 23 ++++++++++++++++++\n",
      "Stopped at epoch:  9\n",
      "4/4 [==============================] - 0s 2ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "32/32 [==============================] - 0s 2ms/step\n",
      "+++++++++++ [1_1] | 24 ++++++++++++++++++\n",
      "Stopped at epoch:  6\n",
      "4/4 [==============================] - 0s 2ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "32/32 [==============================] - 0s 2ms/step\n",
      "+++++++++++ [1_1] | 25 ++++++++++++++++++\n",
      "Stopped at epoch:  6\n",
      "4/4 [==============================] - 0s 2ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 274ms/step\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "32/32 [==============================] - 0s 2ms/step\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>r2</th>\n",
       "      <th>r2_sup</th>\n",
       "      <th>r2_test</th>\n",
       "      <th>r2_val</th>\n",
       "      <th>r2_vt</th>\n",
       "      <th>mse</th>\n",
       "      <th>mse_sup</th>\n",
       "      <th>mse_test</th>\n",
       "      <th>mse_val</th>\n",
       "      <th>mse_vt</th>\n",
       "      <th>mape</th>\n",
       "      <th>rmse</th>\n",
       "      <th>r2_adj</th>\n",
       "      <th>rsd</th>\n",
       "      <th>aic</th>\n",
       "      <th>bic</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>model_1_1_13</th>\n",
       "      <td>0.963833</td>\n",
       "      <td>0.602613</td>\n",
       "      <td>0.928706</td>\n",
       "      <td>0.962173</td>\n",
       "      <td>0.951915</td>\n",
       "      <td>0.013305</td>\n",
       "      <td>0.235906</td>\n",
       "      <td>0.014272</td>\n",
       "      <td>0.014657</td>\n",
       "      <td>0.014464</td>\n",
       "      <td>0.051441</td>\n",
       "      <td>0.115348</td>\n",
       "      <td>0.935417</td>\n",
       "      <td>0.116274</td>\n",
       "      <td>118.639213</td>\n",
       "      <td>274.634718</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>model_1_1_15</th>\n",
       "      <td>0.968140</td>\n",
       "      <td>0.600234</td>\n",
       "      <td>0.944046</td>\n",
       "      <td>0.965469</td>\n",
       "      <td>0.959142</td>\n",
       "      <td>0.011721</td>\n",
       "      <td>0.237318</td>\n",
       "      <td>0.011201</td>\n",
       "      <td>0.013380</td>\n",
       "      <td>0.012291</td>\n",
       "      <td>0.049746</td>\n",
       "      <td>0.108262</td>\n",
       "      <td>0.943108</td>\n",
       "      <td>0.109132</td>\n",
       "      <td>118.892804</td>\n",
       "      <td>274.888309</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>model_1_1_12</th>\n",
       "      <td>0.960696</td>\n",
       "      <td>0.598530</td>\n",
       "      <td>0.922918</td>\n",
       "      <td>0.959447</td>\n",
       "      <td>0.948233</td>\n",
       "      <td>0.014459</td>\n",
       "      <td>0.238330</td>\n",
       "      <td>0.015431</td>\n",
       "      <td>0.015713</td>\n",
       "      <td>0.015572</td>\n",
       "      <td>0.054363</td>\n",
       "      <td>0.120247</td>\n",
       "      <td>0.929814</td>\n",
       "      <td>0.121213</td>\n",
       "      <td>118.472820</td>\n",
       "      <td>274.468325</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>model_1_1_18</th>\n",
       "      <td>0.969501</td>\n",
       "      <td>0.598209</td>\n",
       "      <td>0.947409</td>\n",
       "      <td>0.966499</td>\n",
       "      <td>0.960924</td>\n",
       "      <td>0.011220</td>\n",
       "      <td>0.238520</td>\n",
       "      <td>0.010528</td>\n",
       "      <td>0.012981</td>\n",
       "      <td>0.011754</td>\n",
       "      <td>0.048902</td>\n",
       "      <td>0.105924</td>\n",
       "      <td>0.945538</td>\n",
       "      <td>0.106775</td>\n",
       "      <td>118.980125</td>\n",
       "      <td>274.975630</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>model_1_1_14</th>\n",
       "      <td>0.964725</td>\n",
       "      <td>0.598188</td>\n",
       "      <td>0.937260</td>\n",
       "      <td>0.962401</td>\n",
       "      <td>0.954908</td>\n",
       "      <td>0.012977</td>\n",
       "      <td>0.238533</td>\n",
       "      <td>0.012560</td>\n",
       "      <td>0.014568</td>\n",
       "      <td>0.013564</td>\n",
       "      <td>0.052695</td>\n",
       "      <td>0.113917</td>\n",
       "      <td>0.937009</td>\n",
       "      <td>0.114832</td>\n",
       "      <td>118.689138</td>\n",
       "      <td>274.684643</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>model_1_1_17</th>\n",
       "      <td>0.969296</td>\n",
       "      <td>0.598091</td>\n",
       "      <td>0.947192</td>\n",
       "      <td>0.966303</td>\n",
       "      <td>0.960726</td>\n",
       "      <td>0.011295</td>\n",
       "      <td>0.238590</td>\n",
       "      <td>0.010571</td>\n",
       "      <td>0.013057</td>\n",
       "      <td>0.011814</td>\n",
       "      <td>0.049155</td>\n",
       "      <td>0.106280</td>\n",
       "      <td>0.945172</td>\n",
       "      <td>0.107134</td>\n",
       "      <td>118.966706</td>\n",
       "      <td>274.962211</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>model_1_1_16</th>\n",
       "      <td>0.968938</td>\n",
       "      <td>0.597650</td>\n",
       "      <td>0.946506</td>\n",
       "      <td>0.965923</td>\n",
       "      <td>0.960253</td>\n",
       "      <td>0.011427</td>\n",
       "      <td>0.238852</td>\n",
       "      <td>0.010709</td>\n",
       "      <td>0.013204</td>\n",
       "      <td>0.011956</td>\n",
       "      <td>0.049316</td>\n",
       "      <td>0.106898</td>\n",
       "      <td>0.944532</td>\n",
       "      <td>0.107757</td>\n",
       "      <td>118.943503</td>\n",
       "      <td>274.939008</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>model_1_1_19</th>\n",
       "      <td>0.971797</td>\n",
       "      <td>0.597622</td>\n",
       "      <td>0.945376</td>\n",
       "      <td>0.968928</td>\n",
       "      <td>0.961813</td>\n",
       "      <td>0.010375</td>\n",
       "      <td>0.238869</td>\n",
       "      <td>0.010935</td>\n",
       "      <td>0.012039</td>\n",
       "      <td>0.011487</td>\n",
       "      <td>0.046944</td>\n",
       "      <td>0.101860</td>\n",
       "      <td>0.949637</td>\n",
       "      <td>0.102678</td>\n",
       "      <td>119.136621</td>\n",
       "      <td>275.132126</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>model_1_1_11</th>\n",
       "      <td>0.957914</td>\n",
       "      <td>0.592442</td>\n",
       "      <td>0.918002</td>\n",
       "      <td>0.957304</td>\n",
       "      <td>0.945218</td>\n",
       "      <td>0.015483</td>\n",
       "      <td>0.241944</td>\n",
       "      <td>0.016415</td>\n",
       "      <td>0.016543</td>\n",
       "      <td>0.016479</td>\n",
       "      <td>0.058167</td>\n",
       "      <td>0.124430</td>\n",
       "      <td>0.924846</td>\n",
       "      <td>0.125430</td>\n",
       "      <td>118.336038</td>\n",
       "      <td>274.331543</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>model_1_1_20</th>\n",
       "      <td>0.971550</td>\n",
       "      <td>0.591078</td>\n",
       "      <td>0.939946</td>\n",
       "      <td>0.969120</td>\n",
       "      <td>0.960129</td>\n",
       "      <td>0.010466</td>\n",
       "      <td>0.242754</td>\n",
       "      <td>0.012022</td>\n",
       "      <td>0.011965</td>\n",
       "      <td>0.011994</td>\n",
       "      <td>0.047249</td>\n",
       "      <td>0.102305</td>\n",
       "      <td>0.949196</td>\n",
       "      <td>0.103127</td>\n",
       "      <td>119.119179</td>\n",
       "      <td>275.114683</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>model_1_1_10</th>\n",
       "      <td>0.952772</td>\n",
       "      <td>0.589190</td>\n",
       "      <td>0.908807</td>\n",
       "      <td>0.952211</td>\n",
       "      <td>0.938878</td>\n",
       "      <td>0.017375</td>\n",
       "      <td>0.243874</td>\n",
       "      <td>0.018256</td>\n",
       "      <td>0.018517</td>\n",
       "      <td>0.018386</td>\n",
       "      <td>0.062119</td>\n",
       "      <td>0.131812</td>\n",
       "      <td>0.915664</td>\n",
       "      <td>0.132871</td>\n",
       "      <td>118.105503</td>\n",
       "      <td>274.101007</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>model_1_1_9</th>\n",
       "      <td>0.951924</td>\n",
       "      <td>0.587981</td>\n",
       "      <td>0.904447</td>\n",
       "      <td>0.950447</td>\n",
       "      <td>0.936291</td>\n",
       "      <td>0.017686</td>\n",
       "      <td>0.244592</td>\n",
       "      <td>0.019128</td>\n",
       "      <td>0.019200</td>\n",
       "      <td>0.019164</td>\n",
       "      <td>0.062740</td>\n",
       "      <td>0.132990</td>\n",
       "      <td>0.914150</td>\n",
       "      <td>0.134059</td>\n",
       "      <td>118.069911</td>\n",
       "      <td>274.065416</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>model_1_1_21</th>\n",
       "      <td>0.971076</td>\n",
       "      <td>0.583565</td>\n",
       "      <td>0.930522</td>\n",
       "      <td>0.969426</td>\n",
       "      <td>0.957191</td>\n",
       "      <td>0.010641</td>\n",
       "      <td>0.247214</td>\n",
       "      <td>0.013909</td>\n",
       "      <td>0.011846</td>\n",
       "      <td>0.012877</td>\n",
       "      <td>0.047471</td>\n",
       "      <td>0.103153</td>\n",
       "      <td>0.948351</td>\n",
       "      <td>0.103982</td>\n",
       "      <td>119.086166</td>\n",
       "      <td>275.081671</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>model_1_1_8</th>\n",
       "      <td>0.945411</td>\n",
       "      <td>0.582187</td>\n",
       "      <td>0.892992</td>\n",
       "      <td>0.943924</td>\n",
       "      <td>0.928278</td>\n",
       "      <td>0.020082</td>\n",
       "      <td>0.248032</td>\n",
       "      <td>0.021422</td>\n",
       "      <td>0.021728</td>\n",
       "      <td>0.021575</td>\n",
       "      <td>0.067818</td>\n",
       "      <td>0.141712</td>\n",
       "      <td>0.902520</td>\n",
       "      <td>0.142850</td>\n",
       "      <td>117.815830</td>\n",
       "      <td>273.811335</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>model_1_1_6</th>\n",
       "      <td>0.938071</td>\n",
       "      <td>0.575179</td>\n",
       "      <td>0.880042</td>\n",
       "      <td>0.936648</td>\n",
       "      <td>0.919283</td>\n",
       "      <td>0.022783</td>\n",
       "      <td>0.252192</td>\n",
       "      <td>0.024014</td>\n",
       "      <td>0.024547</td>\n",
       "      <td>0.024280</td>\n",
       "      <td>0.073594</td>\n",
       "      <td>0.150940</td>\n",
       "      <td>0.889412</td>\n",
       "      <td>0.152152</td>\n",
       "      <td>117.563500</td>\n",
       "      <td>273.559005</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>model_1_1_7</th>\n",
       "      <td>0.939005</td>\n",
       "      <td>0.574684</td>\n",
       "      <td>0.881364</td>\n",
       "      <td>0.937718</td>\n",
       "      <td>0.920412</td>\n",
       "      <td>0.022439</td>\n",
       "      <td>0.252486</td>\n",
       "      <td>0.023749</td>\n",
       "      <td>0.024132</td>\n",
       "      <td>0.023941</td>\n",
       "      <td>0.073001</td>\n",
       "      <td>0.149796</td>\n",
       "      <td>0.891081</td>\n",
       "      <td>0.150999</td>\n",
       "      <td>117.593916</td>\n",
       "      <td>273.589421</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>model_1_1_5</th>\n",
       "      <td>0.931502</td>\n",
       "      <td>0.568538</td>\n",
       "      <td>0.869316</td>\n",
       "      <td>0.930166</td>\n",
       "      <td>0.911539</td>\n",
       "      <td>0.025199</td>\n",
       "      <td>0.256134</td>\n",
       "      <td>0.026161</td>\n",
       "      <td>0.027058</td>\n",
       "      <td>0.026610</td>\n",
       "      <td>0.078403</td>\n",
       "      <td>0.158743</td>\n",
       "      <td>0.877682</td>\n",
       "      <td>0.160018</td>\n",
       "      <td>117.361871</td>\n",
       "      <td>273.357376</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>model_1_1_4</th>\n",
       "      <td>0.922114</td>\n",
       "      <td>0.561043</td>\n",
       "      <td>0.851982</td>\n",
       "      <td>0.920486</td>\n",
       "      <td>0.899537</td>\n",
       "      <td>0.028653</td>\n",
       "      <td>0.260584</td>\n",
       "      <td>0.029631</td>\n",
       "      <td>0.030809</td>\n",
       "      <td>0.030220</td>\n",
       "      <td>0.084038</td>\n",
       "      <td>0.169272</td>\n",
       "      <td>0.860918</td>\n",
       "      <td>0.170632</td>\n",
       "      <td>117.104992</td>\n",
       "      <td>273.100497</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>model_1_1_3</th>\n",
       "      <td>0.904356</td>\n",
       "      <td>0.548352</td>\n",
       "      <td>0.815212</td>\n",
       "      <td>0.900848</td>\n",
       "      <td>0.874655</td>\n",
       "      <td>0.035186</td>\n",
       "      <td>0.268118</td>\n",
       "      <td>0.036992</td>\n",
       "      <td>0.038418</td>\n",
       "      <td>0.037705</td>\n",
       "      <td>0.093980</td>\n",
       "      <td>0.187579</td>\n",
       "      <td>0.829207</td>\n",
       "      <td>0.189086</td>\n",
       "      <td>116.694220</td>\n",
       "      <td>272.689725</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>model_1_1_2</th>\n",
       "      <td>0.890360</td>\n",
       "      <td>0.537873</td>\n",
       "      <td>0.788777</td>\n",
       "      <td>0.887123</td>\n",
       "      <td>0.857019</td>\n",
       "      <td>0.040335</td>\n",
       "      <td>0.274339</td>\n",
       "      <td>0.042284</td>\n",
       "      <td>0.043736</td>\n",
       "      <td>0.043010</td>\n",
       "      <td>0.101880</td>\n",
       "      <td>0.200835</td>\n",
       "      <td>0.804215</td>\n",
       "      <td>0.202448</td>\n",
       "      <td>116.421084</td>\n",
       "      <td>272.416589</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>model_1_1_1</th>\n",
       "      <td>0.878597</td>\n",
       "      <td>0.527869</td>\n",
       "      <td>0.766182</td>\n",
       "      <td>0.875179</td>\n",
       "      <td>0.841808</td>\n",
       "      <td>0.044662</td>\n",
       "      <td>0.280277</td>\n",
       "      <td>0.046807</td>\n",
       "      <td>0.048364</td>\n",
       "      <td>0.047586</td>\n",
       "      <td>0.108568</td>\n",
       "      <td>0.211335</td>\n",
       "      <td>0.783208</td>\n",
       "      <td>0.213032</td>\n",
       "      <td>116.217246</td>\n",
       "      <td>272.212751</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>model_1_1_0</th>\n",
       "      <td>0.864403</td>\n",
       "      <td>0.513695</td>\n",
       "      <td>0.739779</td>\n",
       "      <td>0.856793</td>\n",
       "      <td>0.821181</td>\n",
       "      <td>0.049884</td>\n",
       "      <td>0.288692</td>\n",
       "      <td>0.052093</td>\n",
       "      <td>0.055488</td>\n",
       "      <td>0.053791</td>\n",
       "      <td>0.115334</td>\n",
       "      <td>0.223347</td>\n",
       "      <td>0.757863</td>\n",
       "      <td>0.225141</td>\n",
       "      <td>115.996116</td>\n",
       "      <td>271.991621</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>model_1_1_22</th>\n",
       "      <td>0.938433</td>\n",
       "      <td>0.502285</td>\n",
       "      <td>0.515913</td>\n",
       "      <td>0.971016</td>\n",
       "      <td>0.820255</td>\n",
       "      <td>0.022649</td>\n",
       "      <td>0.295465</td>\n",
       "      <td>0.096908</td>\n",
       "      <td>0.011230</td>\n",
       "      <td>0.054069</td>\n",
       "      <td>0.052048</td>\n",
       "      <td>0.150497</td>\n",
       "      <td>0.890059</td>\n",
       "      <td>0.151706</td>\n",
       "      <td>117.575241</td>\n",
       "      <td>273.570746</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>model_1_1_23</th>\n",
       "      <td>0.919819</td>\n",
       "      <td>0.432824</td>\n",
       "      <td>0.286343</td>\n",
       "      <td>0.971296</td>\n",
       "      <td>0.744047</td>\n",
       "      <td>0.029497</td>\n",
       "      <td>0.336700</td>\n",
       "      <td>0.142865</td>\n",
       "      <td>0.011122</td>\n",
       "      <td>0.076993</td>\n",
       "      <td>0.053263</td>\n",
       "      <td>0.171748</td>\n",
       "      <td>0.856819</td>\n",
       "      <td>0.173128</td>\n",
       "      <td>117.046903</td>\n",
       "      <td>273.042408</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>model_1_1_24</th>\n",
       "      <td>0.919801</td>\n",
       "      <td>0.420092</td>\n",
       "      <td>0.282334</td>\n",
       "      <td>0.971561</td>\n",
       "      <td>0.742883</td>\n",
       "      <td>0.029504</td>\n",
       "      <td>0.344258</td>\n",
       "      <td>0.143667</td>\n",
       "      <td>0.011019</td>\n",
       "      <td>0.077343</td>\n",
       "      <td>0.052939</td>\n",
       "      <td>0.171767</td>\n",
       "      <td>0.856788</td>\n",
       "      <td>0.173147</td>\n",
       "      <td>117.046466</td>\n",
       "      <td>273.041971</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    r2    r2_sup   r2_test    r2_val     r2_vt       mse  \\\n",
       "model_1_1_13  0.963833  0.602613  0.928706  0.962173  0.951915  0.013305   \n",
       "model_1_1_15  0.968140  0.600234  0.944046  0.965469  0.959142  0.011721   \n",
       "model_1_1_12  0.960696  0.598530  0.922918  0.959447  0.948233  0.014459   \n",
       "model_1_1_18  0.969501  0.598209  0.947409  0.966499  0.960924  0.011220   \n",
       "model_1_1_14  0.964725  0.598188  0.937260  0.962401  0.954908  0.012977   \n",
       "model_1_1_17  0.969296  0.598091  0.947192  0.966303  0.960726  0.011295   \n",
       "model_1_1_16  0.968938  0.597650  0.946506  0.965923  0.960253  0.011427   \n",
       "model_1_1_19  0.971797  0.597622  0.945376  0.968928  0.961813  0.010375   \n",
       "model_1_1_11  0.957914  0.592442  0.918002  0.957304  0.945218  0.015483   \n",
       "model_1_1_20  0.971550  0.591078  0.939946  0.969120  0.960129  0.010466   \n",
       "model_1_1_10  0.952772  0.589190  0.908807  0.952211  0.938878  0.017375   \n",
       "model_1_1_9   0.951924  0.587981  0.904447  0.950447  0.936291  0.017686   \n",
       "model_1_1_21  0.971076  0.583565  0.930522  0.969426  0.957191  0.010641   \n",
       "model_1_1_8   0.945411  0.582187  0.892992  0.943924  0.928278  0.020082   \n",
       "model_1_1_6   0.938071  0.575179  0.880042  0.936648  0.919283  0.022783   \n",
       "model_1_1_7   0.939005  0.574684  0.881364  0.937718  0.920412  0.022439   \n",
       "model_1_1_5   0.931502  0.568538  0.869316  0.930166  0.911539  0.025199   \n",
       "model_1_1_4   0.922114  0.561043  0.851982  0.920486  0.899537  0.028653   \n",
       "model_1_1_3   0.904356  0.548352  0.815212  0.900848  0.874655  0.035186   \n",
       "model_1_1_2   0.890360  0.537873  0.788777  0.887123  0.857019  0.040335   \n",
       "model_1_1_1   0.878597  0.527869  0.766182  0.875179  0.841808  0.044662   \n",
       "model_1_1_0   0.864403  0.513695  0.739779  0.856793  0.821181  0.049884   \n",
       "model_1_1_22  0.938433  0.502285  0.515913  0.971016  0.820255  0.022649   \n",
       "model_1_1_23  0.919819  0.432824  0.286343  0.971296  0.744047  0.029497   \n",
       "model_1_1_24  0.919801  0.420092  0.282334  0.971561  0.742883  0.029504   \n",
       "\n",
       "               mse_sup  mse_test   mse_val    mse_vt      mape      rmse  \\\n",
       "model_1_1_13  0.235906  0.014272  0.014657  0.014464  0.051441  0.115348   \n",
       "model_1_1_15  0.237318  0.011201  0.013380  0.012291  0.049746  0.108262   \n",
       "model_1_1_12  0.238330  0.015431  0.015713  0.015572  0.054363  0.120247   \n",
       "model_1_1_18  0.238520  0.010528  0.012981  0.011754  0.048902  0.105924   \n",
       "model_1_1_14  0.238533  0.012560  0.014568  0.013564  0.052695  0.113917   \n",
       "model_1_1_17  0.238590  0.010571  0.013057  0.011814  0.049155  0.106280   \n",
       "model_1_1_16  0.238852  0.010709  0.013204  0.011956  0.049316  0.106898   \n",
       "model_1_1_19  0.238869  0.010935  0.012039  0.011487  0.046944  0.101860   \n",
       "model_1_1_11  0.241944  0.016415  0.016543  0.016479  0.058167  0.124430   \n",
       "model_1_1_20  0.242754  0.012022  0.011965  0.011994  0.047249  0.102305   \n",
       "model_1_1_10  0.243874  0.018256  0.018517  0.018386  0.062119  0.131812   \n",
       "model_1_1_9   0.244592  0.019128  0.019200  0.019164  0.062740  0.132990   \n",
       "model_1_1_21  0.247214  0.013909  0.011846  0.012877  0.047471  0.103153   \n",
       "model_1_1_8   0.248032  0.021422  0.021728  0.021575  0.067818  0.141712   \n",
       "model_1_1_6   0.252192  0.024014  0.024547  0.024280  0.073594  0.150940   \n",
       "model_1_1_7   0.252486  0.023749  0.024132  0.023941  0.073001  0.149796   \n",
       "model_1_1_5   0.256134  0.026161  0.027058  0.026610  0.078403  0.158743   \n",
       "model_1_1_4   0.260584  0.029631  0.030809  0.030220  0.084038  0.169272   \n",
       "model_1_1_3   0.268118  0.036992  0.038418  0.037705  0.093980  0.187579   \n",
       "model_1_1_2   0.274339  0.042284  0.043736  0.043010  0.101880  0.200835   \n",
       "model_1_1_1   0.280277  0.046807  0.048364  0.047586  0.108568  0.211335   \n",
       "model_1_1_0   0.288692  0.052093  0.055488  0.053791  0.115334  0.223347   \n",
       "model_1_1_22  0.295465  0.096908  0.011230  0.054069  0.052048  0.150497   \n",
       "model_1_1_23  0.336700  0.142865  0.011122  0.076993  0.053263  0.171748   \n",
       "model_1_1_24  0.344258  0.143667  0.011019  0.077343  0.052939  0.171767   \n",
       "\n",
       "                r2_adj       rsd         aic         bic  \n",
       "model_1_1_13  0.935417  0.116274  118.639213  274.634718  \n",
       "model_1_1_15  0.943108  0.109132  118.892804  274.888309  \n",
       "model_1_1_12  0.929814  0.121213  118.472820  274.468325  \n",
       "model_1_1_18  0.945538  0.106775  118.980125  274.975630  \n",
       "model_1_1_14  0.937009  0.114832  118.689138  274.684643  \n",
       "model_1_1_17  0.945172  0.107134  118.966706  274.962211  \n",
       "model_1_1_16  0.944532  0.107757  118.943503  274.939008  \n",
       "model_1_1_19  0.949637  0.102678  119.136621  275.132126  \n",
       "model_1_1_11  0.924846  0.125430  118.336038  274.331543  \n",
       "model_1_1_20  0.949196  0.103127  119.119179  275.114683  \n",
       "model_1_1_10  0.915664  0.132871  118.105503  274.101007  \n",
       "model_1_1_9   0.914150  0.134059  118.069911  274.065416  \n",
       "model_1_1_21  0.948351  0.103982  119.086166  275.081671  \n",
       "model_1_1_8   0.902520  0.142850  117.815830  273.811335  \n",
       "model_1_1_6   0.889412  0.152152  117.563500  273.559005  \n",
       "model_1_1_7   0.891081  0.150999  117.593916  273.589421  \n",
       "model_1_1_5   0.877682  0.160018  117.361871  273.357376  \n",
       "model_1_1_4   0.860918  0.170632  117.104992  273.100497  \n",
       "model_1_1_3   0.829207  0.189086  116.694220  272.689725  \n",
       "model_1_1_2   0.804215  0.202448  116.421084  272.416589  \n",
       "model_1_1_1   0.783208  0.213032  116.217246  272.212751  \n",
       "model_1_1_0   0.757863  0.225141  115.996116  271.991621  \n",
       "model_1_1_22  0.890059  0.151706  117.575241  273.570746  \n",
       "model_1_1_23  0.856819  0.173128  117.046903  273.042408  \n",
       "model_1_1_24  0.856788  0.173147  117.046466  273.041971  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataFrame salvo em ../results/metrics_1_1\n",
      "+++++++++++ [1_2] | 1 ++++++++++++++++++\n",
      "Stopped at epoch:  6\n",
      "4/4 [==============================] - 0s 3ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "32/32 [==============================] - 0s 2ms/step\n",
      "+++++++++++ [1_2] | 2 ++++++++++++++++++\n",
      "Stopped at epoch:  7\n",
      "4/4 [==============================] - 0s 2ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "32/32 [==============================] - 0s 2ms/step\n",
      "+++++++++++ [1_2] | 3 ++++++++++++++++++\n",
      "Stopped at epoch:  6\n",
      "4/4 [==============================] - 0s 2ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "32/32 [==============================] - 0s 2ms/step\n",
      "+++++++++++ [1_2] | 4 ++++++++++++++++++\n",
      "Stopped at epoch:  6\n",
      "4/4 [==============================] - 0s 2ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "32/32 [==============================] - 0s 2ms/step\n",
      "+++++++++++ [1_2] | 5 ++++++++++++++++++\n",
      "Stopped at epoch:  24\n",
      "4/4 [==============================] - 0s 3ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "32/32 [==============================] - 0s 2ms/step\n",
      "+++++++++++ [1_2] | 6 ++++++++++++++++++\n",
      "Stopped at epoch:  11\n",
      "4/4 [==============================] - 0s 2ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "32/32 [==============================] - 0s 2ms/step\n",
      "+++++++++++ [1_2] | 7 ++++++++++++++++++\n",
      "Stopped at epoch:  13\n",
      "4/4 [==============================] - 0s 2ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "32/32 [==============================] - 0s 2ms/step\n",
      "+++++++++++ [1_2] | 8 ++++++++++++++++++\n",
      "Stopped at epoch:  6\n",
      "4/4 [==============================] - 0s 2ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "32/32 [==============================] - 0s 2ms/step\n",
      "+++++++++++ [1_2] | 9 ++++++++++++++++++\n",
      "Stopped at epoch:  6\n",
      "4/4 [==============================] - 0s 2ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "32/32 [==============================] - 0s 2ms/step\n",
      "+++++++++++ [1_2] | 10 ++++++++++++++++++\n",
      "Stopped at epoch:  16\n",
      "4/4 [==============================] - 0s 2ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "32/32 [==============================] - 0s 2ms/step\n",
      "+++++++++++ [1_2] | 11 ++++++++++++++++++\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_22156/2341861226.py\u001b[0m in \u001b[0;36m?\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m tester = Tester(_df=df_spherical_norm,\n\u001b[1;32m      2\u001b[0m                 \u001b[0m_df_1000\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdf_1000_norm\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m                 run_times=25, dataset_run_times=100)\n\u001b[0;32m----> 4\u001b[0;31m tester.Loop(sort_by='mse',\n\u001b[0m\u001b[1;32m      5\u001b[0m             \u001b[0mboundarie\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.5\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m             \u001b[0mhidden_sizes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m12\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m             \u001b[0mregularizers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0.02\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_22156/1386626785.py\u001b[0m in \u001b[0;36m?\u001b[0;34m(self, sort_by, boundarie, hidden_sizes, regularizers, learning_rate)\u001b[0m\n\u001b[1;32m     57\u001b[0m       \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Testando combinacao{count}: {header}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msetArchitecure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrainer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m       \u001b[0;32mfor\u001b[0m \u001b[0mj\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset_run_times\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m         \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minput_1000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput_1000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 61\u001b[0;31m         \u001b[0;32mif\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLoopWeights\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msort_by\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mboundarie\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrainer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34mf\"{count}_{j}\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     62\u001b[0m           \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSaveDataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrainer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34mf\"dataset_{count}_{j}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m           \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDisplayBetterResults\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'mse_sup'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34mf\"{count}_{j}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbetter_metrics\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_22156/1386626785.py\u001b[0m in \u001b[0;36m?\u001b[0;34m(self, sort_by, boundarie, trainer, idx)\u001b[0m\n\u001b[1;32m     39\u001b[0m     \u001b[0msave\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_times\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m       \u001b[0mprint\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34mf\"+++++++++++ [{idx}] | {i + 1} ++++++++++++++++++\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 43\u001b[0;31m       \u001b[0mmetrics\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrainer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     44\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mmetrics\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0msort_by\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0mboundarie\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;31m# should be >= to acsending metrics\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m         \u001b[0mfileName\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34mf\"model_{idx}_{better_model}\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSaveModelWeights\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfileName\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_22156/1386626785.py\u001b[0m in \u001b[0;36m?\u001b[0;34m(self, trainer, epochs)\u001b[0m\n\u001b[1;32m     23\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mTrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrainer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m     \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_using_lm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_dataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m     \u001b[0;32mreturn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_metrics\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlm_model\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_22156/1784717495.py\u001b[0m in \u001b[0;36m?\u001b[0;34m(self, train_dataset, epochs)\u001b[0m\n\u001b[1;32m     39\u001b[0m       early_stopping_monitor = EarlyStopping(monitor='val_loss',\n\u001b[1;32m     40\u001b[0m                                               \u001b[0mpatience\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m6\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m                                               restore_best_weights=True)\n\u001b[1;32m     42\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 43\u001b[0;31m       self.results = self.lm_model.fit(train_dataset,\n\u001b[0m\u001b[1;32m     44\u001b[0m                                             \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m                                             \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mval_dataset\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m                                             \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mearly_stopping_monitor\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/workspaces/Ic-Rna-2024/VSG/Kriging/Article/Spherical/content/tf-levenberg-marquardt/levenberg_marquardt.py\u001b[0m in \u001b[0;36m?\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, **kwargs)\u001b[0m\n\u001b[1;32m    774\u001b[0m             callbacks.append(tf.keras.callbacks.ProgbarLogger(\n\u001b[1;32m    775\u001b[0m                 \u001b[0mcount_mode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'steps'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    776\u001b[0m                 stateful_metrics=[\"damping_factor\", \"attempts\"]))\n\u001b[1;32m    777\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 778\u001b[0;31m         return super(ModelWrapper, self).fit(\n\u001b[0m\u001b[1;32m    779\u001b[0m             \u001b[0mx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    780\u001b[0m             \u001b[0my\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    781\u001b[0m             \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.python/current/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py\u001b[0m in \u001b[0;36m?\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     68\u001b[0m             \u001b[0;31m# To get the full stack trace, call:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m             \u001b[0;31m# `tf.debugging.disable_traceback_filtering()`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     70\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 72\u001b[0;31m             \u001b[0;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/.python/current/lib/python3.10/site-packages/keras/src/engine/training.py\u001b[0m in \u001b[0;36m?\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1803\u001b[0m                             \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1804\u001b[0m                             \u001b[0m_r\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1805\u001b[0m                         ):\n\u001b[1;32m   1806\u001b[0m                             \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1807\u001b[0;31m                             \u001b[0mtmp_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1808\u001b[0m                             \u001b[0;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1809\u001b[0m                                 \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1810\u001b[0m                             \u001b[0;31m# No error, now safe to assign to logs.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.python/current/lib/python3.10/site-packages/keras/src/engine/training.py\u001b[0m in \u001b[0;36m?\u001b[0;34m(iterator)\u001b[0m\n\u001b[1;32m   1399\u001b[0m             \u001b[0;32mdef\u001b[0m \u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1400\u001b[0m                 \u001b[0;34m\"\"\"Runs a training execution with a single step.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1401\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mstep_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/.python/current/lib/python3.10/site-packages/keras/src/engine/training.py\u001b[0m in \u001b[0;36m?\u001b[0;34m(model, iterator)\u001b[0m\n\u001b[1;32m   1380\u001b[0m                 run_step = tf.function(\n\u001b[1;32m   1381\u001b[0m                     \u001b[0mrun_step\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mjit_compile\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreduce_retracing\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1382\u001b[0m                 )\n\u001b[1;32m   1383\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1384\u001b[0;31m             \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdistribute_strategy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_step\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1385\u001b[0m             outputs = reduce_per_replica(\n\u001b[1;32m   1386\u001b[0m                 \u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1387\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdistribute_strategy\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.python/current/lib/python3.10/site-packages/tensorflow/python/distribute/distribute_lib.py\u001b[0m in \u001b[0;36m?\u001b[0;34m(***failed resolving arguments***)\u001b[0m\n\u001b[1;32m   1677\u001b[0m       \u001b[0;31m# tf.distribute supports Eager functions, so AutoGraph should not be\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1678\u001b[0m       \u001b[0;31m# applied when the caller is also in Eager mode.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1679\u001b[0m       fn = autograph.tf_convert(\n\u001b[1;32m   1680\u001b[0m           fn, autograph_ctx.control_status_ctx(), convert_by_default=False)\n\u001b[0;32m-> 1681\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_extended\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcall_for_each_replica\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/.python/current/lib/python3.10/site-packages/tensorflow/python/distribute/distribute_lib.py\u001b[0m in \u001b[0;36m?\u001b[0;34m(self, fn, args, kwargs)\u001b[0m\n\u001b[1;32m   3267\u001b[0m     \u001b[0m_require_cross_replica_or_default_context_extended\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3268\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3269\u001b[0m       \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3270\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_container_strategy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscope\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3271\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_for_each_replica\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/.python/current/lib/python3.10/site-packages/tensorflow/python/distribute/distribute_lib.py\u001b[0m in \u001b[0;36m?\u001b[0;34m(self, fn, args, kwargs)\u001b[0m\n\u001b[1;32m   4067\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_call_for_each_replica\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4068\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mReplicaContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_container_strategy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreplica_id_in_sync_group\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4069\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/.python/current/lib/python3.10/site-packages/tensorflow/python/autograph/impl/api.py\u001b[0m in \u001b[0;36m?\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    594\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    595\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mag_ctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mControlStatusCtx\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstatus\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mag_ctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mStatus\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mUNSPECIFIED\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 596\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/.python/current/lib/python3.10/site-packages/keras/src/engine/training.py\u001b[0m in \u001b[0;36m?\u001b[0;34m(data)\u001b[0m\n\u001b[1;32m   1372\u001b[0m             \u001b[0;32mdef\u001b[0m \u001b[0mrun_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1373\u001b[0;31m                 \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1374\u001b[0m                 \u001b[0;31m# Ensure counter is updated only if `train_step` succeeds.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1375\u001b[0m                 \u001b[0;32mwith\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontrol_dependencies\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_minimum_control_deps\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1376\u001b[0m                     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_train_counter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0massign_add\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/workspaces/Ic-Rna-2024/VSG/Kriging/Article/Spherical/content/tf-levenberg-marquardt/levenberg_marquardt.py\u001b[0m in \u001b[0;36m?\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m    736\u001b[0m         \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m \u001b[0;34m=\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    737\u001b[0m             \u001b[0mdata_adapter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munpack_x_y_sample_weight\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    738\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    739\u001b[0m         \u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattempts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstop_training\u001b[0m \u001b[0;34m=\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 740\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    741\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    742\u001b[0m         logs = {\"damping_factor\": self.trainer.damping_factor,\n\u001b[1;32m    743\u001b[0m                 \u001b[0;34m\"attempts\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mattempts\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/workspaces/Ic-Rna-2024/VSG/Kriging/Article/Spherical/content/tf-levenberg-marquardt/levenberg_marquardt.py\u001b[0m in \u001b[0;36m?\u001b[0;34m(self, inputs, targets)\u001b[0m\n\u001b[1;32m    601\u001b[0m         \u001b[0mnum_residuals\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch_size\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_outputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    602\u001b[0m         \u001b[0moverdetermined\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnum_residuals\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_variables\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    603\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    604\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0moverdetermined\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 605\u001b[0;31m             loss, outputs, attempts, stop_training = self._train_step(\n\u001b[0m\u001b[1;32m    606\u001b[0m                 \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    607\u001b[0m                 \u001b[0mtargets\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    608\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_init_gauss_newton_overdetermined\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/workspaces/Ic-Rna-2024/VSG/Kriging/Article/Spherical/content/tf-levenberg-marquardt/levenberg_marquardt.py\u001b[0m in \u001b[0;36m?\u001b[0;34m(self, inputs, targets, init_gauss_newton, compute_gauss_newton)\u001b[0m\n\u001b[1;32m    520\u001b[0m                 \u001b[0;31m# Compute the updates:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    521\u001b[0m                 \u001b[0;31m# overdetermined: updates = (J'*J + damping)^-1*J'*residuals\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    522\u001b[0m                 \u001b[0;31m# underdetermined: updates = J'*(J*J' + damping)^-1*residuals\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    523\u001b[0m                 \u001b[0mupdates\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompute_gauss_newton\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mJ\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mJJ_damped\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrhs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 524\u001b[0;31m             \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    525\u001b[0m                 \u001b[0;32mdel\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    526\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    527\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreduce_all\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_finite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mupdates\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/workspaces/Ic-Rna-2024/VSG/Kriging/Article/Spherical/content/tf-levenberg-marquardt/levenberg_marquardt.py\u001b[0m in \u001b[0;36m?\u001b[0;34m(self, J, JJ, rhs)\u001b[0m\n\u001b[1;32m    478\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_compute_gauss_newton_overdetermined\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mJ\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mJJ\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrhs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 479\u001b[0;31m         \u001b[0mupdates\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msolve_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mJJ\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrhs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    480\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mupdates\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/workspaces/Ic-Rna-2024/VSG/Kriging/Article/Spherical/content/tf-levenberg-marquardt/levenberg_marquardt.py\u001b[0m in \u001b[0;36m?\u001b[0;34m(matrix, rhs)\u001b[0m\n\u001b[1;32m    342\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mqr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmatrix\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrhs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    343\u001b[0m             \u001b[0mq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinalg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mqr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmatrix\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfull_matrices\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 344\u001b[0;31m             \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinalg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmatmul\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrhs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtranspose_a\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    345\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinalg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtriangular_solve\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlower\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.python/current/lib/python3.10/site-packages/tensorflow/python/ops/weak_tensor_ops.py\u001b[0m in \u001b[0;36m?\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    140\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    141\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_auto_dtype_conversion_enabled\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 142\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    143\u001b[0m     \u001b[0mbound_arguments\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msignature\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    144\u001b[0m     \u001b[0mbound_arguments\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_defaults\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    145\u001b[0m     \u001b[0mbound_kwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbound_arguments\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marguments\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.python/current/lib/python3.10/site-packages/tensorflow/python/util/traceback_utils.py\u001b[0m in \u001b[0;36m?\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    151\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    153\u001b[0m       \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    154\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 155\u001b[0;31m       \u001b[0;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/.python/current/lib/python3.10/site-packages/tensorflow/python/util/dispatch.py\u001b[0m in \u001b[0;36m?\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m   1257\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1258\u001b[0m       \u001b[0;31m# Fallback dispatch system (dispatch v1):\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1259\u001b[0m       \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1260\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mdispatch_target\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1261\u001b[0;31m       \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mTypeError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1262\u001b[0m         \u001b[0;31m# Note: convert_to_eager_tensor currently raises a ValueError, not a\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1263\u001b[0m         \u001b[0;31m# TypeError, when given unexpected types.  So we need to catch both.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1264\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdispatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mop_dispatch_handler\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.python/current/lib/python3.10/site-packages/tensorflow/python/ops/math_ops.py\u001b[0m in \u001b[0;36m?\u001b[0;34m(a, b, transpose_a, transpose_b, adjoint_a, adjoint_b, a_is_sparse, b_is_sparse, output_type, name)\u001b[0m\n\u001b[1;32m   3838\u001b[0m         \u001b[0madjoint_b\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0madjoint_b\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mtranspose_b\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3839\u001b[0m         return gen_math_ops.batch_mat_mul_v3(\n\u001b[1;32m   3840\u001b[0m             a, b, adj_x=adjoint_a, adj_y=adjoint_b, Tout=output_type, name=name)\n\u001b[1;32m   3841\u001b[0m       \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3842\u001b[0;31m         return gen_math_ops.mat_mul(\n\u001b[0m\u001b[1;32m   3843\u001b[0m             a, b, transpose_a=transpose_a, transpose_b=transpose_b, name=name)\n",
      "\u001b[0;32m~/.python/current/lib/python3.10/site-packages/tensorflow/python/ops/gen_math_ops.py\u001b[0m in \u001b[0;36m?\u001b[0;34m(a, b, transpose_a, transpose_b, name)\u001b[0m\n\u001b[1;32m   6173\u001b[0m         transpose_b)\n\u001b[1;32m   6174\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0m_result\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6175\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0m_core\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6176\u001b[0m       \u001b[0m_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraise_from_not_ok_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 6177\u001b[0;31m     \u001b[0;32mexcept\u001b[0m \u001b[0m_core\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_FallbackException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   6178\u001b[0m       \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6179\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6180\u001b[0m       return mat_mul_eager_fallback(\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "tester = Tester(_df=df_spherical_norm,\n",
    "                _df_1000=df_1000_norm,\n",
    "                run_times=25, dataset_run_times=100)\n",
    "tester.Loop(sort_by='mse',\n",
    "            boundarie = 0.5,\n",
    "            hidden_sizes = [[2, 12]],\n",
    "            regularizers=[0.02],\n",
    "            learning_rate=[0.01])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
